{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-O3O9atNoFK",
        "outputId": "e9aece31-dd7f-4803-c831-4ceed1da26ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Phishing'...\n",
            "remote: Enumerating objects: 238, done.\u001b[K\n",
            "remote: Counting objects: 100% (238/238), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 238 (delta 112), reused 215 (delta 89), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (238/238), 9.41 MiB | 12.23 MiB/s, done.\n",
            "Resolving deltas: 100% (112/112), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/Mandalor-09/Phishing.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Phishing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBSn597DOWUH",
        "outputId": "7f7aa302-7798-4671-cb89-1472bf3e223c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Phishing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7WNK1vQ7OjoO",
        "outputId": "cfe0ed2d-8a33-40f1-89d0-2e1296910693"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Phishing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3fIzjVHOmkq",
        "outputId": "9b420f56-502e-45c2-d2be-236923e790fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: stopit, watchdog, validators, smmap, python-whois, deap, update-checker, pydeck, gitdb, tpot, gitpython, streamlit, Phishing-Website-Detection\n",
            "  Running setup.py develop for Phishing-Website-Detection\n",
            "Successfully installed Phishing-Website-Detection deap-1.4.1 gitdb-4.0.11 gitpython-3.1.41 pydeck-0.8.1b0 python-whois-0.8.0 smmap-5.0.1 stopit-1.1.2 streamlit-1.30.0 tpot-0.12.1 update-checker-0.18.0 validators-0.22.0 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/components/data_ingestion.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej0w1LSLM6k9",
        "outputId": "937f1a43-f4a4-4866-9390-dc0c70554a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artifacts/cleaned_data/final_data.csv\n",
            "artifacts/cleaned_data/train_data.csv\n",
            "artifacts/cleaned_data/test_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/components/model_trainer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP0Eu8NRO9_M",
        "outputId": "3aed785e-8d1c-406d-cdf5-008146009fbc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artifacts/cleaned_data/final_data.csv\n",
            "artifacts/cleaned_data/train_data.csv\n",
            "artifacts/cleaned_data/test_data.csv\n",
            "artifacts/components/standard.joblib\n",
            "artifacts/components/pca.joblib\n",
            "Model Training Initiated\n",
            "                                                                \n",
            "Generation 1 - Current best internal CV score: 0.9363916672933744\n",
            "                                                                \n",
            "Generation 2 - Current best internal CV score: 0.9364969542001955\n",
            "                                                                \n",
            "Generation 3 - Current best internal CV score: 0.9364969542001955\n",
            "                                                                \n",
            "Generation 4 - Current best internal CV score: 0.9364969542001955\n",
            "                                                                \n",
            "Generation 5 - Current best internal CV score: 0.9364969542001955\n",
            "\n",
            "Best pipeline: XGBClassifier(CombineDFs(input_matrix, input_matrix), colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_depth=7, min_child_weight=1, n_estimators=356, subsample=0.9)\n",
            "Model Training Completed\n",
            "Test Data Loaded Successfully\n",
            "Standardization Model loaded Successfully\n",
            "PCA Model loaded Successfully\n",
            "Test Accuracy: 0.9354300153415757\n",
            "artifacts/model/model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Directory to zip (modify this based on your directory structure)\n",
        "directory_to_zip = '/content/Phishing'\n",
        "\n",
        "# Create a zip file\n",
        "zip_file_path = '/content/Phishing.zip'\n",
        "shutil.make_archive(zip_file_path[:-4], 'zip', directory_to_zip)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GOtNFBSMZMr0",
        "outputId": "2fc79552-2197-4a34-a5cc-fb4ecb40fc02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Phishing.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Testing on Small df**"
      ],
      "metadata": {
        "id": "d1mHuOFgOwPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "JIwpsuGYO_Xn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_df = pd.read_csv('/content/Phishing/data/dataset_small.csv')\n",
        "scaler = joblib.load('/content/Phishing/artifacts/components/standard.joblib')\n",
        "pca = joblib.load('/content/Phishing/artifacts/components/pca.joblib')\n",
        "tpot = joblib.load('/content/Phishing/artifacts/model/model.joblib')"
      ],
      "metadata": {
        "id": "kbrrLJ65bADl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/content/Phishing/artifacts/cleaned_data/train_data.csv')\n",
        "train_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA6k5REaTYyq",
        "outputId": "cb6deb07-f0ca-4988-a281-fb4ed088cabb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['qty_dot_url', 'qty_hyphen_url', 'qty_underline_url', 'qty_slash_url',\n",
              "       'qty_questionmark_url', 'qty_equal_url', 'qty_at_url',\n",
              "       'qty_exclamation_url', 'qty_space_url', 'qty_tilde_url',\n",
              "       'qty_comma_url', 'qty_plus_url', 'qty_asterisk_url', 'qty_hashtag_url',\n",
              "       'qty_dollar_url', 'qty_percent_url', 'qty_tld_url', 'length_url',\n",
              "       'qty_dot_domain', 'qty_hyphen_domain', 'qty_underline_domain',\n",
              "       'qty_at_domain', 'qty_vowels_domain', 'domain_in_ip',\n",
              "       'server_client_domain', 'qty_dot_directory', 'qty_hyphen_directory',\n",
              "       'qty_underline_directory', 'qty_percent_directory', 'directory_length',\n",
              "       'file_length', 'qty_dot_params', 'qty_hyphen_params',\n",
              "       'qty_underline_params', 'qty_slash_params', 'qty_questionmark_params',\n",
              "       'qty_percent_params', 'email_in_url', 'tls_ssl_certificate',\n",
              "       'url_shortened', 'phishing'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = small_df[list(train_data.columns)]\n",
        "X = df.drop('phishing',axis=1)\n",
        "y = df['phishing']"
      ],
      "metadata": {
        "id": "yUFAkeplQCfq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "standard_data = scaler.transform(X)\n",
        "pca_data = pca.transform(standard_data)\n",
        "ypred = tpot.predict(pca_data)\n",
        "accuracy = accuracy_score(y,ypred)\n",
        "accuracy2 = tpot.score(pca_data,y)\n",
        "accuracy,accuracy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHo5tRuFQ7sx",
        "outputId": "3cf6095e-da56-4ff5-f42c-bd52a874a483"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9287236763577458, 0.9287236763577458)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "\n",
        "def divide_url(url):\n",
        "    # Parse the URL\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Extract components\n",
        "    domain = parsed_url.netloc\n",
        "    directory = parsed_url.path\n",
        "    file = directory.split('/')[-1]\n",
        "    parameters = parsed_url.query\n",
        "\n",
        "    # Print the divided components\n",
        "    print(f\"Domain: {domain}\")\n",
        "    print(f\"Directory: {directory}\")\n",
        "    print(f\"File: {file}\")\n",
        "    print(f\"Parameters: {parameters}\")\n",
        "\n",
        "# Example usage\n",
        "url = 'https://www.example.org/products/category1/productA/details?color=blue&size=medium'\n",
        "divide_url(url)\n"
      ],
      "metadata": {
        "id": "fmlMmQADc8Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86a67c7-7957-42d7-a0fe-ebc581028ffe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domain: www.example.org\n",
            "Directory: /products/category1/productA/details\n",
            "File: details\n",
            "Parameters: color=blue&size=medium\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import joblib\n",
        "import socket\n",
        "\n",
        "def is_ip_address(domain):\n",
        "    try:\n",
        "        socket.inet_aton(domain)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_additional_url_features(url):\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    return {\n",
        "        'qty_dot_url': url.count('.'),\n",
        "        'qty_hyphen_url': url.count('-'),\n",
        "        'qty_underline_url': url.count('_'),\n",
        "        'qty_slash_url': url.count('/'),\n",
        "        'qty_questionmark_url': url.count('?'),\n",
        "        'qty_equal_url': url.count('='),\n",
        "        'qty_at_url': url.count('@'),\n",
        "        'qty_exclamation_url': url.count('!'),\n",
        "        'qty_space_url': url.count(' '),\n",
        "        'qty_tilde_url': url.count('~'),\n",
        "        'qty_comma_url': url.count(','),\n",
        "        'qty_plus_url': url.count('+'),\n",
        "        'qty_asterisk_url': url.count('*'),\n",
        "        'qty_hashtag_url': url.count('#'),\n",
        "        'qty_dollar_url': url.count('$'),\n",
        "        'qty_percent_url': url.count('%'),\n",
        "        'qty_tld_url': len(parsed_url.netloc.split('.')[-1]),\n",
        "        'length_url': len(url)\n",
        "    }\n",
        "\n",
        "def extract_additional_domain_features(url):\n",
        "    # Parse the URL to get the domain\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    if not domain:\n",
        "        return {\n",
        "            'qty_dot_domain': -1,\n",
        "            'qty_hyphen_domain': -1,\n",
        "            'qty_underline_domain': -1,\n",
        "            'qty_at_domain': -1,\n",
        "            'qty_vowels_domain': -1,\n",
        "            'domain_in_ip': -1,\n",
        "            'server_client_domain': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'qty_dot_domain': domain.count('.'),\n",
        "        'qty_hyphen_domain': domain.count('-'),\n",
        "        'qty_underline_domain': domain.count('_'),\n",
        "        'qty_at_domain': domain.count('@'),\n",
        "        'qty_vowels_domain': sum(1 for char in domain if char.lower() in \"aeiou\"),\n",
        "        'domain_in_ip': 1 if is_ip_address(domain) else 0,\n",
        "        'server_client_domain': 1 if domain.startswith(\"www.\") else 0\n",
        "    }\n",
        "\n",
        "def extract_additional_path_features(url):\n",
        "    # Parse the URL to get the path\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    if not path:\n",
        "        return {\n",
        "            'qty_dot_directory': -1,\n",
        "            'qty_hyphen_directory': -1,\n",
        "            'qty_underline_directory': -1,\n",
        "            'qty_percent_directory': -1,\n",
        "            'directory_length': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'qty_dot_directory': path.count('.'),\n",
        "        'qty_hyphen_directory': path.count('-'),\n",
        "        'qty_underline_directory': path.count('_'),\n",
        "        'qty_percent_directory': path.count('%'),\n",
        "        'directory_length': len(path)\n",
        "    }\n",
        "\n",
        "def extract_file_features(url):\n",
        "    # Parse the URL to get the path\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    if not path:\n",
        "        return {\n",
        "            'file_length': -1\n",
        "        }\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = path.split('/')[-1]\n",
        "\n",
        "    # Attribute: Length of the file name\n",
        "    file_length = len(file_name)\n",
        "\n",
        "    return {\n",
        "        'file_length': file_length\n",
        "    }\n",
        "\n",
        "def extract_additional_params_features(url):\n",
        "    # Parse the URL to get the query parameters\n",
        "    query_params = urlparse(url).query\n",
        "\n",
        "    if not query_params:\n",
        "        return {\n",
        "            'qty_dot_params': -1,\n",
        "            'qty_hyphen_params': -1,\n",
        "            'qty_underline_params': -1,\n",
        "            'qty_slash_params': -1,\n",
        "            'qty_questionmark_params': -1,\n",
        "            'qty_percent_params': -1\n",
        "        }\n",
        "\n",
        "    # Extract parameter names from the query string\n",
        "    param_names = parse_qs(query_params).keys()\n",
        "\n",
        "    return {\n",
        "        'qty_dot_params': sum(param.count('.') for param in param_names),\n",
        "        'qty_hyphen_params': sum(param.count('-') for param in param_names),\n",
        "        'qty_underline_params': sum(param.count('_') for param in param_names),\n",
        "        'qty_slash_params': sum(param.count('/') for param in param_names),\n",
        "        'qty_questionmark_params': sum(param.count('?') for param in param_names),\n",
        "        'qty_percent_params': sum(param.count('%') for param in param_names)\n",
        "    }\n",
        "\n",
        "def email_urlshorten(url):\n",
        "    # Parse the URL\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Extract the domain from the URL\n",
        "    domain = parsed_url.netloc\n",
        "\n",
        "    if not domain:\n",
        "        return {\n",
        "            'email_in_url': -1,\n",
        "            'tls_ssl_certificate' : -1,\n",
        "            'url_shortened': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'email_in_url': 1 if '@' in url else 0,\n",
        "        'tls_ssl_certificate' : 1 if url.startswith(\"https://\") else 0,\n",
        "        'url_shortened': 1 if domain in ['bit.ly', 'goo.gl', 'tinyurl.com', 'ow.ly'] else 0\n",
        "    }\n",
        "\n",
        "def extract_all_features(url):\n",
        "    # Extract URL-based features\n",
        "    url_features = extract_additional_url_features(url)\n",
        "\n",
        "    # Extract Domain-based features\n",
        "    domain_features = extract_additional_domain_features(url)\n",
        "\n",
        "    # Extract Page-based features\n",
        "    path_features = extract_additional_path_features(url)\n",
        "\n",
        "    # Extract File-based feature\n",
        "    file_feature = extract_file_features(url)\n",
        "\n",
        "    # Extract Params-based features\n",
        "    params_features = extract_additional_params_features(url)\n",
        "\n",
        "    # Extract Additional Features\n",
        "    additional_features = email_urlshorten(url)\n",
        "\n",
        "    # Combine all features\n",
        "    all_features = {**url_features, **domain_features, **path_features, **file_feature, **params_features, **additional_features}\n",
        "\n",
        "    return all_features\n",
        "\n",
        "# Example usage\n",
        "url = \"https://google.com\"\n",
        "features = extract_all_features(url)\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0_Ylgy2FVps",
        "outputId": "5537a7f4-eef9-404a-9ae8-b140f36d6c92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'qty_dot_url': 1, 'qty_hyphen_url': 0, 'qty_underline_url': 0, 'qty_slash_url': 2, 'qty_questionmark_url': 0, 'qty_equal_url': 0, 'qty_at_url': 0, 'qty_exclamation_url': 0, 'qty_space_url': 0, 'qty_tilde_url': 0, 'qty_comma_url': 0, 'qty_plus_url': 0, 'qty_asterisk_url': 0, 'qty_hashtag_url': 0, 'qty_dollar_url': 0, 'qty_percent_url': 0, 'qty_tld_url': 3, 'length_url': 18, 'qty_dot_domain': 1, 'qty_hyphen_domain': 0, 'qty_underline_domain': 0, 'qty_at_domain': 0, 'qty_vowels_domain': 4, 'domain_in_ip': 0, 'server_client_domain': 0, 'qty_dot_directory': -1, 'qty_hyphen_directory': -1, 'qty_underline_directory': -1, 'qty_percent_directory': -1, 'directory_length': -1, 'file_length': -1, 'qty_dot_params': -1, 'qty_hyphen_params': -1, 'qty_underline_params': -1, 'qty_slash_params': -1, 'qty_questionmark_params': -1, 'qty_percent_params': -1, 'email_in_url': 0, 'tls_ssl_certificate': 1, 'url_shortened': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(extracted_features):\n",
        "    #loaded_pipeline = joblib.load('/content/tpot_xgbclassifier_pipeline.joblib')\n",
        "    #print(extracted_features.values(),len(extracted_features.values()))\n",
        "    data = np.array(list(extracted_features.values())).reshape(1, -1)\n",
        "\n",
        "    # Assuming you have a scaler object\n",
        "    #scaler = joblib.load('artifacts\\components\\standard.joblib')\n",
        "    scaled_data = scaler.transform(data)\n",
        "\n",
        "    # Assuming you have a PCA object\n",
        "    #pca = joblib.load('artifacts\\components\\pca.joblib')\n",
        "    pca_transformed_data = pca.transform(scaled_data)\n",
        "\n",
        "    # Use the trained XGBBoost for prediction\n",
        "    #prediction = loaded_pipeline.predict(pca_transformed_data)\n",
        "    #tpot = joblib.load('artifacts\\model\\model.joblib')\n",
        "    pred = tpot.predict(pca_transformed_data)\n",
        "    return pred\n",
        "\n",
        "pred = prediction(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UsBz907GcpW",
        "outputId": "08ade227-0c61-46e6-d019-30a58a3a3bfe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN4UzU2VG0YU",
        "outputId": "66ecdcc4-ea61-4674-cdb4-47b55182ceff"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9o7wcdQJ23t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}