{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYT68GABxVot"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score,fbeta_score,precision_score,recall_score,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 72ptz43s9v-1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff9qilIayBIW",
        "outputId": "ed4f7b12-1c16-4708-d1f0-87067877182a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  72ptz43s9v-1.zip\n",
            "replace dataset_small.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: dataset_small.csv       \n",
            "replace dataset_full.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: dataset_full.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset_full.csv')"
      ],
      "metadata": {
        "id": "0YpisUPiyI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlhYzkWGyXRu",
        "outputId": "abf57295-2f49-4a50-dfde-1492daa000ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 112)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skipped_features = [\n",
        "    'time_response',\n",
        "    'domain_spf',\n",
        "    'asn_ip',\n",
        "    'qty_ip_resolved',\n",
        "    'qty_nameservers',\n",
        "    'qty_mx_servers',\n",
        "    'ttl_hostname',\n",
        "    'qty_redirects',\n",
        "    'url_google_index',\n",
        "    'domain_google_index', #remove url shorten and https one\n",
        "    'time_domain_activation',\n",
        "    'time_domain_expiration',\n",
        "]\n",
        "\n",
        "\n",
        "df = df.drop(skipped_features,axis=1)\n",
        "\n",
        "class DataCleaning:\n",
        "\n",
        "    def __init__(self, df, missing_threshold, corr_threshold):\n",
        "        self.df = df\n",
        "        self.missing_threshold = missing_threshold\n",
        "        self.corr_threshold = corr_threshold\n",
        "\n",
        "    def col_with_variance_0(self):\n",
        "        columns_to_drop = []\n",
        "        numerical_columns = [col for col in self.df.columns if self.df[col].dtype != 'O']\n",
        "        for col in numerical_columns:\n",
        "            if self.df[col].std() == 0:\n",
        "                columns_to_drop.append(col)\n",
        "        return columns_to_drop\n",
        "\n",
        "    def get_redundant_cols(self):\n",
        "        cols_missing_ratios = self.df.isna().sum().div(self.df.shape[0])\n",
        "        cols_to_drop = list(cols_missing_ratios[cols_missing_ratios > self.missing_threshold].index)\n",
        "        return cols_to_drop\n",
        "\n",
        "    def dropping_columns_on_basis_of_correlation(self):\n",
        "        columns_to_drop = set()\n",
        "        relation = self.df.corr()\n",
        "        for columns in range(len(relation.columns)):\n",
        "            for rows in range(columns):\n",
        "                if abs(relation.iloc[columns, rows]) > self.corr_threshold:\n",
        "                    col_name = relation.columns[columns]\n",
        "                    columns_to_drop.add(col_name)\n",
        "        columns_to_drop = list(columns_to_drop)\n",
        "        return columns_to_drop\n",
        "\n",
        "    def feature_scaling_df(self):\n",
        "        cols_to_drop_1 = self.get_redundant_cols()\n",
        "        cols_to_drop_2 = self.col_with_variance_0()\n",
        "        cols_to_drop_3 = self.dropping_columns_on_basis_of_correlation()\n",
        "        columns_to_drop = cols_to_drop_1 + cols_to_drop_2 + cols_to_drop_3\n",
        "        columns_to_drop = set(columns_to_drop)\n",
        "        return columns_to_drop\n",
        "\n",
        "\n",
        "clean = DataCleaning(df, 0.8, 0.8)\n",
        "drop_columns = clean.feature_scaling_df()\n",
        "print(drop_columns)\n",
        "df2 = df.drop(columns=drop_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN-oORLIyaFI",
        "outputId": "fafd4843-dfc7-46a3-b291-1560421dfaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'qty_underline_file', 'qty_plus_params', 'qty_space_domain', 'domain_length', 'qty_questionmark_file', 'qty_asterisk_domain', 'qty_params', 'qty_space_params', 'tld_present_params', 'qty_plus_file', 'qty_tilde_domain', 'qty_hashtag_domain', 'qty_exclamation_domain', 'qty_dollar_domain', 'qty_hyphen_file', 'qty_at_file', 'qty_equal_params', 'qty_plus_directory', 'qty_and_directory', 'qty_and_params', 'qty_at_directory', 'qty_space_file', 'qty_hashtag_directory', 'qty_asterisk_params', 'qty_slash_domain', 'qty_dollar_directory', 'qty_dot_file', 'qty_hashtag_params', 'qty_exclamation_directory', 'qty_dollar_params', 'qty_questionmark_domain', 'qty_equal_domain', 'qty_asterisk_directory', 'qty_asterisk_file', 'params_length', 'qty_comma_directory', 'qty_tilde_params', 'qty_percent_domain', 'qty_equal_file', 'qty_equal_directory', 'qty_and_url', 'qty_dollar_file', 'qty_space_directory', 'qty_questionmark_directory', 'qty_tilde_file', 'qty_hashtag_file', 'qty_tilde_directory', 'qty_comma_params', 'qty_comma_file', 'qty_plus_domain', 'qty_at_params', 'qty_exclamation_params', 'qty_slash_file', 'qty_exclamation_file', 'qty_slash_directory', 'qty_and_domain', 'qty_and_file', 'qty_comma_domain', 'qty_percent_file'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqJjdurFyr8O",
        "outputId": "0d670f6d-8c27-4a0f-cfe8-13c2cb7feca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2[df2.duplicated(subset=None, keep='first')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "qJaFDGZyyubR",
        "outputId": "191c2a10-a401-478b-ab82-82f5f1c1782d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  qty_underline_url  qty_slash_url  \\\n",
              "64               2               0                  0              0   \n",
              "70               2               0                  0              0   \n",
              "81               2               0                  0              0   \n",
              "87               2               0                  0              0   \n",
              "111              2               0                  0              0   \n",
              "...            ...             ...                ...            ...   \n",
              "88640            2               0                  0              0   \n",
              "88641            2               1                  0              0   \n",
              "88643            2               0                  0              0   \n",
              "88645            2               0                  0              1   \n",
              "88646            2               0                  0              0   \n",
              "\n",
              "       qty_questionmark_url  qty_equal_url  qty_at_url  qty_exclamation_url  \\\n",
              "64                        0              0           0                    0   \n",
              "70                        0              0           0                    0   \n",
              "81                        0              0           0                    0   \n",
              "87                        0              0           0                    0   \n",
              "111                       0              0           0                    0   \n",
              "...                     ...            ...         ...                  ...   \n",
              "88640                     0              0           0                    0   \n",
              "88641                     0              0           0                    0   \n",
              "88643                     0              0           0                    0   \n",
              "88645                     0              0           0                    0   \n",
              "88646                     0              0           0                    0   \n",
              "\n",
              "       qty_space_url  qty_tilde_url  ...  qty_dot_params  qty_hyphen_params  \\\n",
              "64                 0              0  ...              -1                 -1   \n",
              "70                 0              0  ...              -1                 -1   \n",
              "81                 0              0  ...              -1                 -1   \n",
              "87                 0              0  ...              -1                 -1   \n",
              "111                0              0  ...              -1                 -1   \n",
              "...              ...            ...  ...             ...                ...   \n",
              "88640              0              0  ...              -1                 -1   \n",
              "88641              0              0  ...              -1                 -1   \n",
              "88643              0              0  ...              -1                 -1   \n",
              "88645              0              0  ...              -1                 -1   \n",
              "88646              0              0  ...              -1                 -1   \n",
              "\n",
              "       qty_underline_params  qty_slash_params  qty_questionmark_params  \\\n",
              "64                       -1                -1                       -1   \n",
              "70                       -1                -1                       -1   \n",
              "81                       -1                -1                       -1   \n",
              "87                       -1                -1                       -1   \n",
              "111                      -1                -1                       -1   \n",
              "...                     ...               ...                      ...   \n",
              "88640                    -1                -1                       -1   \n",
              "88641                    -1                -1                       -1   \n",
              "88643                    -1                -1                       -1   \n",
              "88645                    -1                -1                       -1   \n",
              "88646                    -1                -1                       -1   \n",
              "\n",
              "       qty_percent_params  email_in_url  tls_ssl_certificate  url_shortened  \\\n",
              "64                     -1             0                    0              0   \n",
              "70                     -1             0                    1              0   \n",
              "81                     -1             0                    0              0   \n",
              "87                     -1             0                    0              0   \n",
              "111                    -1             0                    0              0   \n",
              "...                   ...           ...                  ...            ...   \n",
              "88640                  -1             0                    1              0   \n",
              "88641                  -1             0                    1              0   \n",
              "88643                  -1             0                    0              0   \n",
              "88645                  -1             0                    1              0   \n",
              "88646                  -1             0                    0              0   \n",
              "\n",
              "       phishing  \n",
              "64            0  \n",
              "70            0  \n",
              "81            0  \n",
              "87            0  \n",
              "111           0  \n",
              "...         ...  \n",
              "88640         0  \n",
              "88641         0  \n",
              "88643         0  \n",
              "88645         1  \n",
              "88646         0  \n",
              "\n",
              "[59331 rows x 41 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-172059b9-6049-4372-8e08-70f8b8fa7b08\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>...</th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>email_in_url</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>url_shortened</th>\n",
              "      <th>phishing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88640</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88641</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88643</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88645</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88646</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>59331 rows × 41 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-172059b9-6049-4372-8e08-70f8b8fa7b08')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-172059b9-6049-4372-8e08-70f8b8fa7b08 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-172059b9-6049-4372-8e08-70f8b8fa7b08');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1853d9ae-8204-485d-969d-612fc54b3749\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1853d9ae-8204-485d-969d-612fc54b3749')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1853d9ae-8204-485d-969d-612fc54b3749 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.drop_duplicates()"
      ],
      "metadata": {
        "id": "sfHX-RMly0Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdjfgtHs0NAZ",
        "outputId": "7635f6a9-5628-4b02-9456-e6479ca2a11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29316, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phished = df3[df3['phishing'] == 1]\n",
        "not_phished = df3[df3['phishing'] == 0]"
      ],
      "metadata": {
        "id": "fzIQuJyY0Yg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phished.shape,not_phished.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovnWzU8H0l9P",
        "outputId": "752290b6-ad5c-4a63-8b35-ddb7e398b9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22477, 41), (6839, 41))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets upsample everything"
      ],
      "metadata": {
        "id": "nFSbzb1x0vVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "resample_phished = resample(phished,replace= True,n_samples=45000,random_state=42)\n",
        "resample_not_phished = resample(not_phished,replace= True,n_samples=45000,random_state=42)"
      ],
      "metadata": {
        "id": "3YaVFP9m0nzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resample_phished.shape,resample_not_phished.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdygpnhO3KMW",
        "outputId": "9c06fefe-bc17-4816-cad2-9f9bf8e7ad3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45000, 41), (45000, 41))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = pd.concat([resample_phished,resample_not_phished],axis=0)"
      ],
      "metadata": {
        "id": "CJ1zHnb83k_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ_YwH2P3y14",
        "outputId": "5d674538-a9e1-4c2d-d1b6-bf62d45a3c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90000, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_final.sample(df_final.shape[0])"
      ],
      "metadata": {
        "id": "1Z5OXCVD4eAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_final.drop('phishing',axis=1)\n",
        "y = df_final['phishing']"
      ],
      "metadata": {
        "id": "_TqffpX94LM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape ,y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmvzTs2N5I7L",
        "outputId": "05622dac-796c-4e20-fb49-3385cfc71a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((90000, 40), (90000,))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
      ],
      "metadata": {
        "id": "oYktvij-4qG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LHGG6Km5Xc9",
        "outputId": "dd4bcbae-3ec1-42b6-d2a0-d775e9ef1fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((72000, 40), (72000,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=7)\n",
        "x_pca_train = pca.fit_transform(x_train)\n",
        "x_pca_test = pca.transform(x_test)\n",
        "\n",
        "std = StandardScaler()\n",
        "x_std_train = std.fit_transform(x_pca_train)\n",
        "x_std_test = std.transform(x_pca_test)"
      ],
      "metadata": {
        "id": "hVtOb0lT3VJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DNKVts1BQ1L",
        "outputId": "5d892cce-692e-48cd-aaf9-306399559d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "# Create a K-Means instance\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "\n",
        "# Define a custom scoring function (using silhouette score as an example)\n",
        "def silhouette_scorer(estimator, x_std_train):\n",
        "    labels = estimator.fit_predict(x_std_train)\n",
        "    score = silhouette_score(x_std_train, labels)\n",
        "    return score\n",
        "\n",
        "# Use cross_val_score with the custom scorer\n",
        "scores = cross_val_score(kmeans, x_std_train, cv=5, scoring=silhouette_scorer)\n",
        "\n",
        "# Display the silhouette scores for each fold\n",
        "print(\"Silhouette Scores:\", scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P8klS6x_jW5",
        "outputId": "b28cea75-6691-46d5-f611-d0d129ca320e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Scores: [0.683006   0.24858379 0.88707569 0.62039518 0.27480739]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "list_model = [\n",
        "    LogisticRegression(), SVC(), DecisionTreeClassifier(), RandomForestClassifier(),\n",
        "    GradientBoostingClassifier(), XGBClassifier(), GaussianNB(), LGBMClassifier(), KNeighborsClassifier()\n",
        "]\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "details = {}\n",
        "best_model = None\n",
        "f_accuracy = 0\n",
        "\n",
        "for model in list_model:\n",
        "    cv_results = cross_val_score(model, x_std_train, y_train, cv=cv, scoring='accuracy')\n",
        "    accuracy = np.mean(cv_results)\n",
        "    print(f'{model.__class__.__name__} accuracy = {accuracy}')\n",
        "    details[model.__class__.__name__] = {'accuracy': accuracy, 'cv_results': cv_results}\n",
        "\n",
        "    if accuracy > f_accuracy:\n",
        "        f_accuracy = accuracy\n",
        "        best_model = model.__class__.__name__\n",
        "\n",
        "print(\"Best Model:\", best_model)\n",
        "print(\"Details:\", details)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tFMA6Xv6Fxr",
        "outputId": "0422f102-d157-4664-aac8-f4749f0daad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression accuracy = 0.6927638888888888\n",
            "SVC accuracy = 0.7612916666666667\n",
            "DecisionTreeClassifier accuracy = 0.95\n",
            "RandomForestClassifier accuracy = 0.9573472222222221\n",
            "GradientBoostingClassifier accuracy = 0.7786666666666668\n",
            "XGBClassifier accuracy = 0.8825833333333334\n",
            "GaussianNB accuracy = 0.5902222222222223\n",
            "[LightGBM] [Info] Number of positive: 28814, number of negative: 28786\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003264 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500243 -> initscore=0.000972\n",
            "[LightGBM] [Info] Start training from score 0.000972\n",
            "[LightGBM] [Info] Number of positive: 28814, number of negative: 28786\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002926 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500243 -> initscore=0.000972\n",
            "[LightGBM] [Info] Start training from score 0.000972\n",
            "[LightGBM] [Info] Number of positive: 28814, number of negative: 28786\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002601 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500243 -> initscore=0.000972\n",
            "[LightGBM] [Info] Start training from score 0.000972\n",
            "[LightGBM] [Info] Number of positive: 28815, number of negative: 28785\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002892 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500260 -> initscore=0.001042\n",
            "[LightGBM] [Info] Start training from score 0.001042\n",
            "[LightGBM] [Info] Number of positive: 28815, number of negative: 28785\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002652 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 57600, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500260 -> initscore=0.001042\n",
            "[LightGBM] [Info] Start training from score 0.001042\n",
            "LGBMClassifier accuracy = 0.8337777777777777\n",
            "KNeighborsClassifier accuracy = 0.8853333333333333\n",
            "Best Model: RandomForestClassifier\n",
            "Details: {'LogisticRegression': {'accuracy': 0.6927638888888888, 'cv_results': array([0.69354167, 0.69034722, 0.689375  , 0.69222222, 0.69833333])}, 'SVC': {'accuracy': 0.7612916666666667, 'cv_results': array([0.76027778, 0.76090278, 0.7575    , 0.76027778, 0.7675    ])}, 'DecisionTreeClassifier': {'accuracy': 0.95, 'cv_results': array([0.94993056, 0.94743056, 0.94840278, 0.95      , 0.95423611])}, 'RandomForestClassifier': {'accuracy': 0.9573472222222221, 'cv_results': array([0.95916667, 0.95451389, 0.954375  , 0.956875  , 0.96180556])}, 'GradientBoostingClassifier': {'accuracy': 0.7786666666666668, 'cv_results': array([0.77833333, 0.77888889, 0.77416667, 0.77576389, 0.78618056])}, 'XGBClassifier': {'accuracy': 0.8825833333333334, 'cv_results': array([0.88229167, 0.88659722, 0.87722222, 0.88152778, 0.88527778])}, 'GaussianNB': {'accuracy': 0.5902222222222223, 'cv_results': array([0.591875  , 0.58625   , 0.59034722, 0.59284722, 0.58979167])}, 'LGBMClassifier': {'accuracy': 0.8337777777777777, 'cv_results': array([0.83201389, 0.83270833, 0.83194444, 0.83416667, 0.83805556])}, 'KNeighborsClassifier': {'accuracy': 0.8853333333333333, 'cv_results': array([0.88152778, 0.88229167, 0.88694444, 0.885625  , 0.89027778])}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(details).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "BJ1PS52U-t2w",
        "outputId": "b82276b1-08a4-4de9-b46d-cf3bb42f273b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            accuracy  \\\n",
              "LogisticRegression          0.692764   \n",
              "SVC                         0.761292   \n",
              "DecisionTreeClassifier          0.95   \n",
              "RandomForestClassifier      0.957347   \n",
              "GradientBoostingClassifier  0.778667   \n",
              "XGBClassifier               0.882583   \n",
              "GaussianNB                  0.590222   \n",
              "LGBMClassifier              0.833778   \n",
              "KNeighborsClassifier        0.885333   \n",
              "\n",
              "                                                                   cv_results  \n",
              "LogisticRegression          [0.6935416666666666, 0.6903472222222222, 0.689...  \n",
              "SVC                         [0.7602777777777778, 0.7609027777777778, 0.757...  \n",
              "DecisionTreeClassifier      [0.9499305555555555, 0.9474305555555556, 0.948...  \n",
              "RandomForestClassifier      [0.9591666666666666, 0.9545138888888889, 0.954...  \n",
              "GradientBoostingClassifier  [0.7783333333333333, 0.7788888888888889, 0.774...  \n",
              "XGBClassifier               [0.8822916666666667, 0.8865972222222223, 0.877...  \n",
              "GaussianNB                  [0.591875, 0.58625, 0.5903472222222222, 0.5928...  \n",
              "LGBMClassifier              [0.8320138888888889, 0.8327083333333334, 0.831...  \n",
              "KNeighborsClassifier        [0.8815277777777778, 0.8822916666666667, 0.886...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7697a481-c7e7-4d44-8b12-1b95f3b63125\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>cv_results</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LogisticRegression</th>\n",
              "      <td>0.692764</td>\n",
              "      <td>[0.6935416666666666, 0.6903472222222222, 0.689...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVC</th>\n",
              "      <td>0.761292</td>\n",
              "      <td>[0.7602777777777778, 0.7609027777777778, 0.757...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeClassifier</th>\n",
              "      <td>0.95</td>\n",
              "      <td>[0.9499305555555555, 0.9474305555555556, 0.948...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestClassifier</th>\n",
              "      <td>0.957347</td>\n",
              "      <td>[0.9591666666666666, 0.9545138888888889, 0.954...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradientBoostingClassifier</th>\n",
              "      <td>0.778667</td>\n",
              "      <td>[0.7783333333333333, 0.7788888888888889, 0.774...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBClassifier</th>\n",
              "      <td>0.882583</td>\n",
              "      <td>[0.8822916666666667, 0.8865972222222223, 0.877...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GaussianNB</th>\n",
              "      <td>0.590222</td>\n",
              "      <td>[0.591875, 0.58625, 0.5903472222222222, 0.5928...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LGBMClassifier</th>\n",
              "      <td>0.833778</td>\n",
              "      <td>[0.8320138888888889, 0.8327083333333334, 0.831...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNeighborsClassifier</th>\n",
              "      <td>0.885333</td>\n",
              "      <td>[0.8815277777777778, 0.8822916666666667, 0.886...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7697a481-c7e7-4d44-8b12-1b95f3b63125')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7697a481-c7e7-4d44-8b12-1b95f3b63125 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7697a481-c7e7-4d44-8b12-1b95f3b63125');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a0141fb-507a-459d-a1e0-06ee9868bcf4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a0141fb-507a-459d-a1e0-06ee9868bcf4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a0141fb-507a-459d-a1e0-06ee9868bcf4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params_grid = [\n",
        "    {\n",
        "        'model': XGBClassifier(),\n",
        "        'param_grid': {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'model': KNeighborsClassifier(),\n",
        "        'param_grid': {\n",
        "            'n_neighbors': [3, 5, 7],\n",
        "            'weights': ['uniform', 'distance'],\n",
        "            'p': [1, 2]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'model': RandomForestClassifier(),\n",
        "        'param_grid': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "best_model = None\n",
        "best_param = None\n",
        "best_model_accuracy = 0\n",
        "best_model_info = {}\n",
        "\n",
        "for model_info in params_grid:\n",
        "    model = model_info['model']\n",
        "    param_grid = model_info['param_grid']\n",
        "\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "    grid_search.fit(x_std_train, y_train)\n",
        "\n",
        "    model_name = model.__class__.__name__\n",
        "\n",
        "    print(f\"Best parameters for {model_name}:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    # Get the best accuracy, parameters, precision, recall, and F2 score for the current model\n",
        "    best_accuracy = grid_search.best_score_\n",
        "    best_parameters = grid_search.best_params_\n",
        "\n",
        "    y_pred = grid_search.predict(x_std_test)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "    # Store the results in the best_model_info dictionary\n",
        "    best_model_info[model_name] = {\n",
        "        'best_parameters': best_parameters,\n",
        "        'best_accuracy': best_accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f2_score': f2_score\n",
        "    }\n",
        "\n",
        "    # Update the best_model, best_param, and best_model_accuracy if the current model performed better\n",
        "    if best_accuracy > best_model_accuracy:\n",
        "        best_model = model_name\n",
        "        best_param = best_parameters\n",
        "        best_model_accuracy = best_accuracy\n",
        "\n",
        "print(\"\\nBest Model:\", best_model)\n",
        "print(\"Best Parameters:\", best_param)\n",
        "print(\"Best Model Accuracy:\", best_model_accuracy)\n",
        "\n",
        "# Print the best parameters, accuracy, precision, recall, and F2 score for each model\n",
        "print(\"\\nBest Results for Each Model:\")\n",
        "for model_name, info in best_model_info.items():\n",
        "    print(f\"{model_name}:\")\n",
        "    print(f\"  Best Parameters: {info['best_parameters']}\")\n",
        "    print(f\"  Best Accuracy: {info['best_accuracy']:.4f}\")\n",
        "    print(f\"  Precision: {info['precision']:.4f}\")\n",
        "    print(f\"  Recall: {info['recall']:.4f}\")\n",
        "    print(f\"  F2 Score: {info['f2_score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c34mYQGVNspW",
        "outputId": "f6c34ad1-0f92-490e-b15b-b08d4ae8c62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for XGBClassifier:\n",
            "{'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 300}\n",
            "Best parameters for KNeighborsClassifier:\n",
            "{'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/hyperopt/hyperopt-sklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "EftEXXBJ-LHL",
        "outputId": "223e5c34-cc03-4b65-838c-b8fb925d1f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/hyperopt/hyperopt-sklearn\n",
            "  Cloning https://github.com/hyperopt/hyperopt-sklearn to /tmp/pip-req-build-x11q0b45\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/hyperopt/hyperopt-sklearn /tmp/pip-req-build-x11q0b45\n",
            "  Resolved https://github.com/hyperopt/hyperopt-sklearn to commit 4bc286479677a0bfd2178dac4546ea268b3f3b77\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: hyperopt>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from hpsklearn==1.0.3) (0.2.7)\n",
            "Collecting numpy>=1.26.0 (from hpsklearn==1.0.3)\n",
            "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.3.0 (from hpsklearn==1.0.3)\n",
            "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from hpsklearn==1.0.3) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (0.10.9.7)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn==1.0.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn==1.0.3) (3.2.0)\n",
            "Building wheels for collected packages: hpsklearn\n",
            "  Building wheel for hpsklearn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hpsklearn: filename=hpsklearn-1.0.3-py3-none-any.whl size=135342 sha256=f5b6fe225bf2addf71afaa860ab2a4b97a93b4fb52d94e5e494c71bd56498255\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0z2f26ho/wheels/01/e8/01/ad06c89501e4845c988d4e846f45f3485d9b60be0b9ebea43b\n",
            "Successfully built hpsklearn\n",
            "Installing collected packages: numpy, scikit-learn, hpsklearn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hpsklearn-1.0.3 numpy-1.26.3 scikit-learn-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade hpsklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-ctCUEUFbbz",
        "outputId": "3cc6bd5a-41b8-4c56-fbed-a0d8a7dc8c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hpsklearn in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: hyperopt>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from hpsklearn) (0.2.7)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from hpsklearn) (1.26.3)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from hpsklearn) (1.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from hpsklearn) (1.11.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (3.2.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (0.18.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn) (0.10.9.7)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show hpsklearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLZ_d375OASG",
        "outputId": "c5219053-fe74-4590-d074-0345c727e90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: hpsklearn\n",
            "Version: 1.0.3\n",
            "Summary: Hyperparameter Optimization for sklearn\n",
            "Home-page: http://hyperopt.github.com/hyperopt-sklearn/\n",
            "Author: James Bergstra\n",
            "Author-email: anon@anon.com\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: hyperopt, numpy, scikit-learn, scipy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
        "from hyperopt import tpe\n",
        "from hpsklearn import HyperoptEstimator, any_preprocessing, all_classifiers\n",
        "\n",
        "# Define HyperoptEstimator with all classifiers\n",
        "estim = HyperoptEstimator(\n",
        "    classifier=all_classifiers('my_classifier'),\n",
        "    preprocessing=any_preprocessing('my_pre'),\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trial_timeout=120,\n",
        ")\n",
        "\n",
        "# Fit the estimator\n",
        "estim.fit(x_std_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = estim.best_model()\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = best_model.predict(x_std_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(f\"Test Set Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F2 Score: {f2_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "16qpeT0DMR6t",
        "outputId": "1cfe546d-6b33-41b1-cacf-32a16e0e33ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.19trial/s, best loss: 0.3014583333333334]\n",
            "100%|██████████| 2/2 [01:50<00:00, 110.43s/trial, best loss: 0.2789583333333333]\n",
            "100%|██████████| 3/3 [00:00<00:00,  1.83trial/s, best loss: 0.2789583333333333]\n",
            "100%|██████████| 4/4 [00:05<00:00,  5.32s/trial, best loss: 0.24965277777777772]\n",
            "100%|██████████| 5/5 [00:31<00:00, 31.52s/trial, best loss: 0.24965277777777772]\n",
            "100%|██████████| 6/6 [00:01<00:00,  1.95s/trial, best loss: 0.24965277777777772]\n",
            "100%|██████████| 7/7 [00:07<00:00,  7.52s/trial, best loss: 0.24965277777777772]\n",
            "100%|██████████| 8/8 [00:00<00:00,  2.87trial/s, best loss: 0.24965277777777772]\n",
            "100%|██████████| 9/9 [00:00<00:00,  5.22trial/s, best loss: 0.24965277777777772]\n",
            "100%|██████████| 10/10 [00:00<00:00,  2.98trial/s, best loss: 0.24965277777777772]\n",
            "100%|██████████| 11/11 [01:22<00:00, 82.08s/trial, best loss: 0.24965277777777772]\n",
            "100%|██████████| 12/12 [00:00<00:00,  2.32trial/s, best loss: 0.24965277777777772]\n",
            "100%|██████████| 13/13 [02:00<00:00, 120.39s/trial, best loss: 0.24965277777777772]\n",
            " 93%|█████████▎| 13/14 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/semi_supervised/_label_propagation.py:322: ConvergenceWarning: max_iter=19 was reached without convergence.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 14/14 [00:01<00:00,  1.18s/trial, best loss: 0.14500000000000002]\n",
            " 93%|█████████▎| 14/15 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:hyperopt.fmin:job exception: Negative values in data passed to CategoricalNB (input X)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r 93%|█████████▎| 14/15 [00:00<?, ?trial/s, best loss=?]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Negative values in data passed to CategoricalNB (input X)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5200eab5b565>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Fit the estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mestim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_std_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Get the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hpsklearn/estimator/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 increment = min(self.fit_increment,\n\u001b[1;32m    479\u001b[0m                                 adjusted_max_evals - len(self.trials.trials))\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mfit_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_increment_dump_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hpsklearn/estimator/estimator.py\u001b[0m in \u001b[0;36mfit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Workaround for rstate issue #35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"rstate\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 hyperopt.fmin(_fn_with_timeout,\n\u001b[0m\u001b[1;32m    348\u001b[0m                               \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                               \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# next line is where the fmin is actually executed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                     \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"job exception: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             )\n\u001b[0;32m--> 892\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hpsklearn/estimator/estimator.py\u001b[0m in \u001b[0;36m_fn_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"return\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mfn_rval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# -- remove potentially large objects from the rval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hpsklearn/estimator/_cost_fn.py\u001b[0m in \u001b[0;36m_cost_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m                 )\n\u001b[1;32m    210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXEXfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myfit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mn_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m         \"\"\"\n\u001b[0;32m-> 1378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 )\n\u001b[1;32m   1350\u001b[0m             ):\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m()\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \"\"\"\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1437\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m         )\n\u001b[0;32m-> 1439\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CategoricalNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1572\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to CategoricalNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
        "from hyperopt import tpe\n",
        "from hpsklearn import HyperoptEstimator,any_preprocessing,random_forest_classifier,extra_tree_classifier,bagging_classifier,ada_boost_classifier,gradient_boosting_classifier,hist_gradient_boosting_classifier,ridge_classifier_cv,perceptron,decision_tree_classifier,k_neighbors_classifier,xgboost_classification\n",
        "\n",
        "# Define classifiers\n",
        "hypertopt_classification_estimators = [\n",
        "    random_forest_classifier('my_rf'),\n",
        "    extra_tree_classifier('my_et'),\n",
        "    bagging_classifier('my_bag'),\n",
        "    ada_boost_classifier('my_ada'),\n",
        "    gradient_boosting_classifier('my_gb'),\n",
        "    hist_gradient_boosting_classifier('my_hgb'),\n",
        "    ridge_classifier_cv('my_ridge'),\n",
        "    perceptron('my_perceptron'),\n",
        "    decision_tree_classifier('my_dt'),\n",
        "    k_neighbors_classifier('my_knn'),\n",
        "    xgboost_classification('my_xgb'),\n",
        "]\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for classifier in hypertopt_classification_estimators:\n",
        "    print(f\"\\nClassifier: {classifier}\")\n",
        "\n",
        "    # Create HyperoptEstimator\n",
        "    estim = HyperoptEstimator(\n",
        "        classifier=classifier,\n",
        "        preprocessing=any_preprocessing('my_pre'),\n",
        "        algo=tpe.suggest,\n",
        "        max_evals=100,\n",
        "        trial_timeout=120,\n",
        "    )\n",
        "\n",
        "    # Fit the estimator\n",
        "    estim.fit(x_std_train, y_train)\n",
        "\n",
        "    # Get the best parameters and score\n",
        "    best_params = estim._best_learner\n",
        "    best_score = estim._best_loss\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = estim.predict(x_std_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "    # Store results in the dictionary\n",
        "    results[classifier] = {\n",
        "        'best_params': best_params,\n",
        "        'best_score': best_score,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f2_score': f2_score,\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    print(f\"Best Score: {best_score}\")\n",
        "    print(f\"Test Set Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F2 Score: {f2_score}\")\n",
        "\n",
        "# Find the best model based on test accuracy\n",
        "best_model = max(results, key=lambda x: results[x]['accuracy'])\n",
        "\n",
        "# Print overall best model\n",
        "print(f\"\\nBest Model Overall: {best_model}\")\n"
      ],
      "metadata": {
        "id": "nzmoOildUq5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa21048-b6d0-42b2-9513-7ad278c98ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifier: 0 sklearn_RandomForestClassifier\n",
            "1  bootstrap =\n",
            "2   switch\n",
            "3     hyperopt_param\n",
            "4       Literal{my_rf.rfc_bootstrap}\n",
            "5       randint\n",
            "6         Literal{2}\n",
            "7     Literal{True}\n",
            "8     Literal{False}\n",
            "9  ccp_alpha =\n",
            "10   Literal{0.0}\n",
            "11  class_weight =\n",
            "12   switch\n",
            "13     hyperopt_param\n",
            "14       Literal{my_rf.rfc_class_weight}\n",
            "15       randint\n",
            "16         Literal{3}\n",
            "17     Literal{balanced}\n",
            "18     Literal{balanced_subsample}\n",
            "19     Literal{None}\n",
            "20  criterion =\n",
            "21   switch\n",
            "22     hyperopt_param\n",
            "23       Literal{my_rf.rfc_criterion}\n",
            "24       randint\n",
            "25         Literal{2}\n",
            "26     Literal{gini}\n",
            "27     Literal{entropy}\n",
            "28  max_depth =\n",
            "29   switch\n",
            "30     hyperopt_param\n",
            "31       Literal{my_rf.rfc_max_depth}\n",
            "32       categorical\n",
            "33         pos_args\n",
            "34           Literal{0.7}\n",
            "35           Literal{0.1}\n",
            "36           Literal{0.1}\n",
            "37           Literal{0.1}\n",
            "38     Literal{None}\n",
            "39     Literal{2}\n",
            "40     Literal{3}\n",
            "41     Literal{4}\n",
            "42  max_features =\n",
            "43   switch\n",
            "44     hyperopt_param\n",
            "45       Literal{my_rf.rfc_max_features}\n",
            "46       categorical\n",
            "47         pos_args\n",
            "48           Literal{0.2}\n",
            "49           Literal{0.1}\n",
            "50           Literal{0.1}\n",
            "51           Literal{0.6}\n",
            "52     Literal{sqrt}\n",
            "53     Literal{log2}\n",
            "54     Literal{None}\n",
            "55     float\n",
            "56       hyperopt_param\n",
            "57         Literal{my_rf.rfc_max_features.frac}\n",
            "58         uniform\n",
            "59           Literal{0.0}\n",
            "60           Literal{1.0}\n",
            "61  max_leaf_nodes =\n",
            "62   switch\n",
            "63     hyperopt_param\n",
            "64       Literal{my_rf.rfc_max_leaf_nodes}\n",
            "65       categorical\n",
            "66         pos_args\n",
            "67           Literal{0.85}\n",
            "68           Literal{0.05}\n",
            "69           Literal{0.05}\n",
            "70           Literal{0.05}\n",
            "71     Literal{None}\n",
            "72     Literal{5}\n",
            "73     Literal{10}\n",
            "74     Literal{15}\n",
            "75  max_samples =\n",
            "76   Literal{None}\n",
            "77  min_impurity_decrease =\n",
            "78   switch\n",
            "79     hyperopt_param\n",
            "80       Literal{my_rf.rfc_min_impurity_decrease}\n",
            "81       categorical\n",
            "82         pos_args\n",
            "83           Literal{0.85}\n",
            "84           Literal{0.05}\n",
            "85           Literal{0.05}\n",
            "86           Literal{0.05}\n",
            "87     Literal{0.0}\n",
            "88     Literal{0.01}\n",
            "89     Literal{0.02}\n",
            "90     Literal{0.05}\n",
            "91  min_samples_leaf =\n",
            "92   switch\n",
            "93     hyperopt_param\n",
            "94       Literal{my_rf.rfc_min_samples_leaf}\n",
            "95       randint\n",
            "96         Literal{2}\n",
            "97     Literal{1}\n",
            "98     int\n",
            "99       float\n",
            "100         hyperopt_param\n",
            "101           Literal{my_rf.rfc_min_samples_leaf.gt1}\n",
            "102           qloguniform\n",
            "103             Literal{0.4054651081081644}\n",
            "104             Literal{3.921973336281314}\n",
            "105             Literal{1}\n",
            "106  min_samples_split =\n",
            "107   switch\n",
            "108     hyperopt_param\n",
            "109       Literal{my_rf.rfc_min_samples_split}\n",
            "110       categorical\n",
            "111         pos_args\n",
            "112           Literal{0.95}\n",
            "113           Literal{0.05}\n",
            "114     Literal{2}\n",
            "115     Literal{3}\n",
            "116  min_weight_fraction_leaf =\n",
            "117   Literal{0.0}\n",
            "118  n_estimators =\n",
            "119   int\n",
            "120     float\n",
            "121       hyperopt_param\n",
            "122         Literal{my_rf.rfc_n_estimators}\n",
            "123         qloguniform\n",
            "124           Literal{2.251291798606495}\n",
            "125           Literal{8.006534220429568}\n",
            "126           Literal{1}\n",
            "127  n_jobs =\n",
            "128   Literal{1}\n",
            "129  oob_score =\n",
            "130   Literal{False}\n",
            "131  random_state =\n",
            "132   hyperopt_param\n",
            "133     Literal{my_rf.rfc_random_state}\n",
            "134     randint\n",
            "135       Literal{5}\n",
            "136  verbose =\n",
            "137   Literal{False}\n",
            "138  warm_start =\n",
            "139   Literal{False}\n",
            "100%|██████████| 1/1 [00:03<00:00,  3.07s/trial, best loss: 0.2685416666666667]\n",
            "100%|██████████| 2/2 [00:04<00:00,  4.17s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 3/3 [00:12<00:00, 12.28s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 4/4 [00:54<00:00, 54.40s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 5/5 [02:00<00:00, 120.18s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 6/6 [01:53<00:00, 113.69s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 7/7 [00:10<00:00, 10.04s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 8/8 [00:26<00:00, 26.91s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 9/9 [00:00<00:00,  1.40trial/s, best loss: 0.04020833333333329]\n",
            "100%|██████████| 10/10 [00:02<00:00,  2.51s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 11/11 [02:00<00:00, 120.17s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 12/12 [00:02<00:00,  2.82s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 13/13 [00:38<00:00, 38.07s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 14/14 [00:06<00:00,  6.16s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 15/15 [01:32<00:00, 92.30s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 16/16 [01:21<00:00, 81.82s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 17/17 [00:02<00:00,  2.43s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 18/18 [02:00<00:00, 120.21s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 19/19 [00:15<00:00, 15.36s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 20/20 [00:02<00:00,  2.82s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 21/21 [00:01<00:00,  1.53s/trial, best loss: 0.04020833333333329]\n",
            "100%|██████████| 22/22 [00:05<00:00,  5.52s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 23/23 [00:04<00:00,  4.42s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 24/24 [00:01<00:00,  1.99s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 25/25 [00:01<00:00,  1.24s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 26/26 [00:05<00:00,  5.40s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 27/27 [00:07<00:00,  7.95s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 28/28 [00:08<00:00,  8.67s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 29/29 [00:36<00:00, 36.89s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 30/30 [00:07<00:00,  7.80s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 31/31 [02:00<00:00, 120.17s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 32/32 [00:14<00:00, 14.66s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 33/33 [00:02<00:00,  2.13s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 34/34 [00:18<00:00, 18.48s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 35/35 [00:04<00:00,  4.85s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 36/36 [00:06<00:00,  6.42s/trial, best loss: 0.03979166666666667]\n",
            "100%|██████████| 37/37 [01:30<00:00, 90.08s/trial, best loss: 0.037708333333333344]\n",
            "100%|██████████| 38/38 [01:27<00:00, 87.56s/trial, best loss: 0.037708333333333344]\n",
            "100%|██████████| 39/39 [02:00<00:00, 120.26s/trial, best loss: 0.037708333333333344]\n",
            "100%|██████████| 40/40 [01:26<00:00, 86.58s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 41/41 [02:00<00:00, 120.22s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 42/42 [00:08<00:00,  8.22s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 43/43 [01:25<00:00, 85.85s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 44/44 [00:20<00:00, 20.53s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 45/45 [00:05<00:00,  6.00s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 46/46 [02:00<00:00, 120.19s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 47/47 [02:00<00:00, 120.12s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 48/48 [00:22<00:00, 22.77s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 49/49 [00:14<00:00, 14.00s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 50/50 [02:00<00:00, 120.15s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 51/51 [00:10<00:00, 10.98s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 52/52 [02:00<00:00, 120.20s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 53/53 [00:10<00:00, 10.67s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 54/54 [01:01<00:00, 61.59s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 55/55 [01:12<00:00, 72.09s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 56/56 [02:00<00:00, 120.23s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 57/57 [00:28<00:00, 28.52s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 58/58 [00:42<00:00, 42.06s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 59/59 [00:06<00:00,  6.08s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 60/60 [02:00<00:00, 120.39s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 61/61 [02:00<00:00, 120.24s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 62/62 [02:00<00:00, 120.19s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 63/63 [00:34<00:00, 34.66s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 64/64 [01:42<00:00, 102.61s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 65/65 [01:18<00:00, 78.86s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 66/66 [01:32<00:00, 92.98s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 67/67 [01:21<00:00, 81.00s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 68/68 [01:44<00:00, 104.99s/trial, best loss: 0.03763888888888889]\n",
            "100%|██████████| 69/69 [00:51<00:00, 51.26s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 70/70 [00:49<00:00, 49.55s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 71/71 [02:00<00:00, 120.23s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 72/72 [00:27<00:00, 27.58s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 73/73 [00:37<00:00, 37.50s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 74/74 [00:37<00:00, 37.88s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 75/75 [00:07<00:00,  7.32s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 76/76 [00:22<00:00, 22.56s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 77/77 [00:08<00:00,  8.33s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 78/78 [00:30<00:00, 30.63s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 79/79 [00:23<00:00, 23.76s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 80/80 [00:14<00:00, 14.03s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 81/81 [00:16<00:00, 16.66s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 82/82 [00:09<00:00,  9.16s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 83/83 [00:16<00:00, 16.90s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 84/84 [01:44<00:00, 104.36s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 85/85 [01:08<00:00, 68.23s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 86/86 [00:10<00:00, 10.02s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 87/87 [01:02<00:00, 62.47s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 88/88 [02:00<00:00, 120.18s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 89/89 [02:00<00:00, 120.19s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 90/90 [00:02<00:00,  2.76s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 91/91 [00:29<00:00, 29.89s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 92/92 [00:52<00:00, 52.51s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 93/93 [00:15<00:00, 15.63s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 94/94 [00:41<00:00, 41.49s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 95/95 [00:34<00:00, 34.76s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 96/96 [02:00<00:00, 120.20s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 97/97 [00:00<00:00,  1.51trial/s, best loss: 0.03756944444444443]\n",
            "100%|██████████| 98/98 [02:00<00:00, 120.22s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 99/99 [02:00<00:00, 120.24s/trial, best loss: 0.03756944444444443]\n",
            "100%|██████████| 100/100 [02:00<00:00, 120.23s/trial, best loss: 0.03756944444444443]\n",
            "Best Parameters: RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
            "                       max_features=0.2013603334303137, n_estimators=473,\n",
            "                       n_jobs=1, random_state=4, verbose=False)\n",
            "Best Score: 0.03756944444444443\n",
            "Test Set Accuracy: 0.9628333333333333\n",
            "Precision: 0.9830097087378641\n",
            "Recall: 0.942173479561316\n",
            "F2 Score: 0.9500670241286864\n",
            "\n",
            "Classifier: 0 sklearn_ExtraTreeClassifier\n",
            "1  ccp_alpha =\n",
            "2   Literal{0.0}\n",
            "3  class_weight =\n",
            "4   Literal{None}\n",
            "5  criterion =\n",
            "6   switch\n",
            "7     hyperopt_param\n",
            "8       Literal{my_et.etc_criterion}\n",
            "9       randint\n",
            "10         Literal{2}\n",
            "11     Literal{gini}\n",
            "12     Literal{entropy}\n",
            "13  max_depth =\n",
            "14   switch\n",
            "15     hyperopt_param\n",
            "16       Literal{my_et.etc_max_depth}\n",
            "17       categorical\n",
            "18         pos_args\n",
            "19           Literal{0.7}\n",
            "20           Literal{0.1}\n",
            "21           Literal{0.1}\n",
            "22           Literal{0.1}\n",
            "23     Literal{None}\n",
            "24     Literal{2}\n",
            "25     Literal{3}\n",
            "26     Literal{4}\n",
            "27  max_features =\n",
            "28   switch\n",
            "29     hyperopt_param\n",
            "30       Literal{my_et.etc_max_features}\n",
            "31       categorical\n",
            "32         pos_args\n",
            "33           Literal{0.2}\n",
            "34           Literal{0.1}\n",
            "35           Literal{0.1}\n",
            "36           Literal{0.6}\n",
            "37     Literal{sqrt}\n",
            "38     Literal{log2}\n",
            "39     Literal{None}\n",
            "40     float\n",
            "41       hyperopt_param\n",
            "42         Literal{my_et.etc_max_features.frac}\n",
            "43         uniform\n",
            "44           Literal{0.0}\n",
            "45           Literal{1.0}\n",
            "46  max_leaf_nodes =\n",
            "47   switch\n",
            "48     hyperopt_param\n",
            "49       Literal{my_et.etc_max_leaf_nodes}\n",
            "50       categorical\n",
            "51         pos_args\n",
            "52           Literal{0.85}\n",
            "53           Literal{0.05}\n",
            "54           Literal{0.05}\n",
            "55           Literal{0.05}\n",
            "56     Literal{None}\n",
            "57     Literal{5}\n",
            "58     Literal{10}\n",
            "59     Literal{15}\n",
            "60  min_impurity_decrease =\n",
            "61   Literal{0.0}\n",
            "62  min_samples_leaf =\n",
            "63   Literal{1}\n",
            "64  min_samples_split =\n",
            "65   Literal{2}\n",
            "66  min_weight_fraction_leaf =\n",
            "67   Literal{0.0}\n",
            "68  random_state =\n",
            "69   hyperopt_param\n",
            "70     Literal{my_et.etc_random_state}\n",
            "71     randint\n",
            "72       Literal{5}\n",
            "73  splitter =\n",
            "74   switch\n",
            "75     hyperopt_param\n",
            "76       Literal{my_et.etc_splitter}\n",
            "77       randint\n",
            "78         Literal{2}\n",
            "79     Literal{best}\n",
            "80     Literal{random}\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.77trial/s, best loss: 0.05076388888888894]\n",
            "100%|██████████| 2/2 [00:00<00:00,  6.19trial/s, best loss: 0.05076388888888894]\n",
            "100%|██████████| 3/3 [00:00<00:00,  5.08trial/s, best loss: 0.050000000000000044]\n",
            "100%|██████████| 4/4 [00:00<00:00,  7.27trial/s, best loss: 0.050000000000000044]\n",
            "100%|██████████| 5/5 [00:00<00:00,  6.71trial/s, best loss: 0.050000000000000044]\n",
            "100%|██████████| 6/6 [00:00<00:00,  1.49trial/s, best loss: 0.04805555555555552]\n",
            "100%|██████████| 7/7 [00:00<00:00,  4.07trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 8/8 [00:00<00:00,  3.73trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 9/9 [00:00<00:00,  3.35trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 10/10 [00:00<00:00,  3.53trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 11/11 [00:00<00:00,  3.93trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 12/12 [00:00<00:00,  2.65trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 13/13 [00:00<00:00,  2.15trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 14/14 [00:00<00:00,  6.82trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 15/15 [00:00<00:00,  3.29trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 16/16 [00:00<00:00,  4.33trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 17/17 [00:00<00:00,  6.98trial/s, best loss: 0.04777777777777781]\n",
            "100%|██████████| 18/18 [00:00<00:00,  4.16trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 19/19 [00:00<00:00,  2.56trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 20/20 [00:00<00:00,  7.73trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 21/21 [00:00<00:00,  7.52trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 22/22 [00:00<00:00,  6.81trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 23/23 [00:00<00:00,  6.75trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 24/24 [00:00<00:00,  4.00trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 25/25 [00:00<00:00,  4.10trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 26/26 [00:00<00:00,  4.53trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 27/27 [00:00<00:00,  4.10trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 28/28 [00:00<00:00,  7.39trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 29/29 [00:00<00:00,  4.05trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 30/30 [00:00<00:00,  4.00trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 31/31 [00:00<00:00,  6.33trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 32/32 [00:00<00:00,  3.94trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 33/33 [00:00<00:00,  7.01trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 34/34 [00:00<00:00,  8.81trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 35/35 [00:00<00:00,  9.51trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 36/36 [00:00<00:00,  5.39trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 37/37 [00:00<00:00,  6.74trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 38/38 [00:00<00:00,  5.11trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 39/39 [00:00<00:00,  3.95trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 40/40 [00:00<00:00,  5.04trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 41/41 [00:00<00:00,  8.76trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 42/42 [00:00<00:00,  7.14trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 43/43 [00:00<00:00,  1.36trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 44/44 [00:00<00:00,  4.88trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 45/45 [00:00<00:00,  4.79trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 46/46 [00:00<00:00,  4.53trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 47/47 [00:00<00:00,  4.05trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 48/48 [00:00<00:00,  3.29trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 49/49 [00:00<00:00,  3.21trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 50/50 [00:00<00:00,  5.86trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 51/51 [00:00<00:00,  6.42trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 52/52 [00:00<00:00,  4.55trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 53/53 [00:00<00:00,  1.89trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 54/54 [00:00<00:00,  4.06trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 55/55 [00:00<00:00,  7.99trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 56/56 [00:00<00:00,  1.74trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 57/57 [00:00<00:00,  6.76trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 58/58 [00:00<00:00,  5.25trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 59/59 [00:00<00:00,  4.72trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 60/60 [00:00<00:00,  4.00trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 61/61 [00:00<00:00,  4.37trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 62/62 [00:00<00:00,  6.28trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 63/63 [00:00<00:00,  9.98trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 64/64 [00:00<00:00,  3.61trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 65/65 [00:00<00:00,  6.79trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 66/66 [00:00<00:00,  3.79trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 67/67 [00:00<00:00,  3.87trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 68/68 [00:00<00:00,  3.89trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 69/69 [00:00<00:00,  4.59trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 70/70 [00:00<00:00,  2.64trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 71/71 [00:00<00:00,  3.85trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 72/72 [00:00<00:00,  3.83trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 73/73 [00:00<00:00,  3.80trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 74/74 [00:00<00:00,  6.67trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 75/75 [00:00<00:00,  5.07trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 76/76 [00:00<00:00,  6.34trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 77/77 [00:00<00:00,  6.33trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 78/78 [00:00<00:00,  6.81trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 79/79 [00:00<00:00,  3.90trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 80/80 [00:00<00:00,  7.24trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 81/81 [00:00<00:00,  5.04trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 82/82 [00:00<00:00,  2.19trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 83/83 [00:00<00:00,  4.15trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 84/84 [00:00<00:00,  3.91trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 85/85 [00:00<00:00,  3.88trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 86/86 [00:00<00:00,  5.99trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 87/87 [00:00<00:00,  6.14trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 88/88 [00:00<00:00,  9.98trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 89/89 [00:00<00:00,  8.51trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 90/90 [00:00<00:00,  1.33trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 91/91 [00:00<00:00,  6.24trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 92/92 [00:00<00:00,  3.14trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 93/93 [00:00<00:00,  4.52trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 94/94 [00:00<00:00,  2.77trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 95/95 [00:00<00:00,  2.07trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 96/96 [00:00<00:00,  3.79trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 97/97 [00:00<00:00,  6.53trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 98/98 [00:00<00:00,  6.00trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 99/99 [00:00<00:00,  1.79trial/s, best loss: 0.0461111111111111]\n",
            "100%|██████████| 100/100 [00:00<00:00,  4.07trial/s, best loss: 0.0461111111111111]\n",
            "Best Parameters: ExtraTreeClassifier(max_features=None, random_state=2)\n",
            "Best Score: 0.0461111111111111\n",
            "Test Set Accuracy: 0.9577222222222223\n",
            "Precision: 0.9909717272511286\n",
            "Recall: 0.9241165392710756\n",
            "F2 Score: 0.9367560526433993\n",
            "\n",
            "Classifier: 0 sklearn_BaggingClassifier\n",
            "1  bootstrap =\n",
            "2   switch\n",
            "3     hyperopt_param\n",
            "4       Literal{my_bag.bc_bootstrap}\n",
            "5       randint\n",
            "6         Literal{2}\n",
            "7     Literal{True}\n",
            "8     Literal{False}\n",
            "9  bootstrap_features =\n",
            "10   switch\n",
            "11     hyperopt_param\n",
            "12       Literal{my_bag.bc_bootstrap_features}\n",
            "13       randint\n",
            "14         Literal{2}\n",
            "15     Literal{True}\n",
            "16     Literal{False}\n",
            "17  estimator =\n",
            "18   Literal{None}\n",
            "19  max_features =\n",
            "20   switch\n",
            "21     hyperopt_param\n",
            "22       Literal{my_bag.bc_max_features}\n",
            "23       categorical\n",
            "24         pos_args\n",
            "25           Literal{0.05}\n",
            "26           Literal{0.15}\n",
            "27           Literal{0.8}\n",
            "28     Literal{0.8}\n",
            "29     Literal{0.9}\n",
            "30     Literal{1.0}\n",
            "31  max_samples =\n",
            "32   switch\n",
            "33     hyperopt_param\n",
            "34       Literal{my_bag.bc_max_samples}\n",
            "35       categorical\n",
            "36         pos_args\n",
            "37           Literal{0.05}\n",
            "38           Literal{0.15}\n",
            "39           Literal{0.8}\n",
            "40     Literal{0.8}\n",
            "41     Literal{0.9}\n",
            "42     Literal{1.0}\n",
            "43  n_estimators =\n",
            "44   switch\n",
            "45     hyperopt_param\n",
            "46       Literal{my_bag.bc_n_estimators}\n",
            "47       categorical\n",
            "48         pos_args\n",
            "49           Literal{0.0625}\n",
            "50           Literal{0.175}\n",
            "51           Literal{0.525}\n",
            "52           Literal{0.175}\n",
            "53           Literal{0.0625}\n",
            "54     Literal{8}\n",
            "55     Literal{9}\n",
            "56     Literal{10}\n",
            "57     Literal{11}\n",
            "58     Literal{12}\n",
            "59  n_jobs =\n",
            "60   Literal{1}\n",
            "61  oob_score =\n",
            "62   Literal{False}\n",
            "63  random_state =\n",
            "64   hyperopt_param\n",
            "65     Literal{my_bag.bc_random_state}\n",
            "66     randint\n",
            "67       Literal{5}\n",
            "68  verbose =\n",
            "69   Literal{False}\n",
            "70  warm_start =\n",
            "71   Literal{False}\n",
            "100%|██████████| 1/1 [00:03<00:00,  3.07s/trial, best loss: 0.04090277777777773]\n",
            "100%|██████████| 2/2 [00:04<00:00,  4.88s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 3/3 [00:03<00:00,  3.26s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 4/4 [00:03<00:00,  3.83s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 5/5 [00:04<00:00,  4.80s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 6/6 [00:04<00:00,  4.93s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 7/7 [00:02<00:00,  2.65s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 8/8 [00:02<00:00,  2.31s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 9/9 [00:02<00:00,  2.95s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 10/10 [00:03<00:00,  3.94s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 11/11 [00:03<00:00,  3.69s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 12/12 [00:02<00:00,  2.98s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 13/13 [00:03<00:00,  3.09s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 14/14 [00:02<00:00,  2.79s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 15/15 [00:03<00:00,  3.43s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 16/16 [00:02<00:00,  2.87s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 17/17 [00:04<00:00,  4.66s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 18/18 [00:04<00:00,  4.11s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 19/19 [00:03<00:00,  3.31s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 20/20 [00:03<00:00,  3.21s/trial, best loss: 0.04062500000000002]\n",
            "100%|██████████| 21/21 [00:04<00:00,  4.05s/trial, best loss: 0.039583333333333304]\n",
            "100%|██████████| 22/22 [00:04<00:00,  4.08s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 23/23 [00:04<00:00,  4.72s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 24/24 [00:04<00:00,  4.15s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 25/25 [00:04<00:00,  4.39s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 26/26 [00:04<00:00,  4.44s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 27/27 [00:04<00:00,  4.16s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 28/28 [00:04<00:00,  4.70s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 29/29 [00:03<00:00,  3.25s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 30/30 [00:02<00:00,  2.95s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 31/31 [00:04<00:00,  4.22s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 32/32 [00:04<00:00,  4.05s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 33/33 [00:04<00:00,  4.01s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 34/34 [00:04<00:00,  4.10s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 35/35 [00:03<00:00,  3.57s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 36/36 [00:03<00:00,  3.38s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 37/37 [00:02<00:00,  2.91s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 38/38 [00:04<00:00,  4.33s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 39/39 [00:04<00:00,  4.64s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 40/40 [00:02<00:00,  2.41s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 41/41 [00:04<00:00,  4.58s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 42/42 [00:04<00:00,  4.48s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 43/43 [00:02<00:00,  2.94s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 44/44 [00:04<00:00,  4.55s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 45/45 [00:05<00:00,  5.09s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 46/46 [00:02<00:00,  2.46s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 47/47 [00:04<00:00,  4.19s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 48/48 [00:04<00:00,  4.59s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 49/49 [00:04<00:00,  4.34s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 50/50 [00:03<00:00,  3.10s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 51/51 [00:04<00:00,  4.03s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 52/52 [00:03<00:00,  3.20s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 53/53 [00:04<00:00,  4.77s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 54/54 [00:02<00:00,  2.56s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 55/55 [00:03<00:00,  3.70s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 56/56 [00:02<00:00,  2.92s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 57/57 [00:04<00:00,  4.53s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 58/58 [00:02<00:00,  2.62s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 59/59 [00:04<00:00,  4.56s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 60/60 [00:03<00:00,  3.97s/trial, best loss: 0.038402777777777786]\n",
            "100%|██████████| 61/61 [00:04<00:00,  4.76s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 62/62 [00:02<00:00,  2.69s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 63/63 [00:05<00:00,  5.06s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 64/64 [00:03<00:00,  3.94s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 65/65 [00:05<00:00,  5.19s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 66/66 [00:03<00:00,  3.90s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 67/67 [00:03<00:00,  3.83s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 68/68 [00:04<00:00,  4.63s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 69/69 [00:04<00:00,  4.05s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 70/70 [00:03<00:00,  3.31s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 71/71 [00:04<00:00,  4.50s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 72/72 [00:04<00:00,  4.19s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 73/73 [00:03<00:00,  3.74s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 74/74 [00:05<00:00,  5.09s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 75/75 [00:04<00:00,  4.38s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 76/76 [00:05<00:00,  5.31s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 77/77 [00:04<00:00,  4.07s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 78/78 [00:03<00:00,  3.35s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 79/79 [00:04<00:00,  4.30s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 80/80 [00:04<00:00,  4.36s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 81/81 [00:03<00:00,  3.20s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 82/82 [00:04<00:00,  4.44s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 83/83 [00:03<00:00,  3.19s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 84/84 [00:04<00:00,  4.65s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 85/85 [00:04<00:00,  4.22s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 86/86 [00:04<00:00,  4.01s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 87/87 [00:05<00:00,  5.05s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 88/88 [00:02<00:00,  2.91s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 89/89 [00:05<00:00,  5.15s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 90/90 [00:03<00:00,  3.81s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 91/91 [00:04<00:00,  4.06s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 92/92 [00:02<00:00,  2.63s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 93/93 [00:03<00:00,  3.50s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 94/94 [00:03<00:00,  3.91s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 95/95 [00:05<00:00,  5.34s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 96/96 [00:03<00:00,  3.89s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 97/97 [00:02<00:00,  2.92s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 98/98 [00:03<00:00,  3.13s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 99/99 [00:03<00:00,  3.66s/trial, best loss: 0.038124999999999964]\n",
            "100%|██████████| 100/100 [00:03<00:00,  3.18s/trial, best loss: 0.038124999999999964]\n",
            "Best Parameters: BaggingClassifier(bootstrap=False, bootstrap_features=True, n_estimators=11,\n",
            "                  n_jobs=1, random_state=2, verbose=False)\n",
            "Best Score: 0.038124999999999964\n",
            "Test Set Accuracy: 0.9667777777777777\n",
            "Precision: 0.9905715283436154\n",
            "Recall: 0.9427273734352498\n",
            "F2 Score: 0.951922861808989\n",
            "\n",
            "Classifier: 0 sklearn_AdaBoostClassifier\n",
            "1  algorithm =\n",
            "2   switch\n",
            "3     hyperopt_param\n",
            "4       Literal{my_ada.ada_boost_algorithm}\n",
            "5       randint\n",
            "6         Literal{2}\n",
            "7     Literal{SAMME}\n",
            "8     Literal{SAMME.R}\n",
            "9  estimator =\n",
            "10   Literal{None}\n",
            "11  learning_rate =\n",
            "12   float\n",
            "13     hyperopt_param\n",
            "14       Literal{my_ada.ada_boost_learning_rate}\n",
            "15       lognormal\n",
            "16         Literal{-4.605170185988091}\n",
            "17         Literal{2.3025850929940455}\n",
            "18  n_estimators =\n",
            "19   int\n",
            "20     float\n",
            "21       hyperopt_param\n",
            "22         Literal{my_ada.ada_boost_n_estimators}\n",
            "23         qloguniform\n",
            "24           Literal{2.3513752571634776}\n",
            "25           Literal{6.908255154023788}\n",
            "26           Literal{1}\n",
            "27  random_state =\n",
            "28   hyperopt_param\n",
            "29     Literal{my_ada.ada_boost_random_state}\n",
            "30     randint\n",
            "31       Literal{5}\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.62s/trial, best loss: 0.29923611111111115]\n",
            " 50%|█████     | 1/2 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  2.64s/trial, best loss: 0.29923611111111115]\n",
            " 67%|██████▋   | 2/3 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 3/3 [00:02<00:00,  2.51s/trial, best loss: 0.29798611111111106]\n",
            "100%|██████████| 4/4 [00:00<00:00,  1.50trial/s, best loss: 0.29798611111111106]\n",
            "100%|██████████| 5/5 [00:01<00:00,  1.39s/trial, best loss: 0.29798611111111106]\n",
            " 83%|████████▎ | 5/6 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 6/6 [00:09<00:00,  9.70s/trial, best loss: 0.29798611111111106]\n",
            "100%|██████████| 7/7 [00:08<00:00,  8.06s/trial, best loss: 0.2885416666666667]\n",
            " 88%|████████▊ | 7/8 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 8/8 [00:03<00:00,  3.45s/trial, best loss: 0.26555555555555554]\n",
            "100%|██████████| 9/9 [00:06<00:00,  6.63s/trial, best loss: 0.26555555555555554]\n",
            "100%|██████████| 10/10 [00:06<00:00,  6.87s/trial, best loss: 0.26555555555555554]\n",
            "100%|██████████| 11/11 [00:00<00:00,  1.23trial/s, best loss: 0.26555555555555554]\n",
            " 92%|█████████▏| 11/12 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 12/12 [00:12<00:00, 12.87s/trial, best loss: 0.26555555555555554]\n",
            " 92%|█████████▏| 12/13 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 13/13 [00:03<00:00,  3.27s/trial, best loss: 0.26555555555555554]\n",
            " 93%|█████████▎| 13/14 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 14/14 [00:01<00:00,  1.60s/trial, best loss: 0.26555555555555554]\n",
            " 93%|█████████▎| 14/15 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  4.72s/trial, best loss: 0.26555555555555554]\n",
            "100%|██████████| 16/16 [00:03<00:00,  3.84s/trial, best loss: 0.26555555555555554]\n",
            " 94%|█████████▍| 16/17 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 17/17 [00:03<00:00,  3.49s/trial, best loss: 0.26555555555555554]\n",
            " 94%|█████████▍| 17/18 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 18/18 [00:03<00:00,  3.75s/trial, best loss: 0.26555555555555554]\n",
            " 95%|█████████▍| 18/19 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00,  1.33trial/s, best loss: 0.26555555555555554]\n",
            " 95%|█████████▌| 19/20 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 20/20 [00:15<00:00, 15.42s/trial, best loss: 0.26555555555555554]\n",
            " 95%|█████████▌| 20/21 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 21/21 [00:01<00:00,  1.44s/trial, best loss: 0.26555555555555554]\n",
            " 95%|█████████▌| 21/22 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 22/22 [00:01<00:00,  1.40s/trial, best loss: 0.26555555555555554]\n",
            " 96%|█████████▌| 22/23 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 23/23 [00:01<00:00,  1.19s/trial, best loss: 0.26555555555555554]\n",
            " 96%|█████████▌| 23/24 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 24/24 [00:01<00:00,  1.04s/trial, best loss: 0.26555555555555554]\n",
            " 96%|█████████▌| 24/25 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 25/25 [00:33<00:00, 33.99s/trial, best loss: 0.24236111111111114]\n",
            " 96%|█████████▌| 25/26 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 26/26 [00:44<00:00, 44.76s/trial, best loss: 0.24236111111111114]\n",
            " 96%|█████████▋| 26/27 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:646: RuntimeWarning: overflow encountered in exp\n",
            "  sample_weight *= np.exp(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1351: UserWarning: Sample weights have reached infinite values, at iteration 1, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00,  3.48trial/s, best loss: 0.24236111111111114]\n",
            " 96%|█████████▋| 27/28 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 28/28 [00:35<00:00, 35.43s/trial, best loss: 0.24236111111111114]\n",
            " 97%|█████████▋| 28/29 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 29/29 [00:23<00:00, 23.64s/trial, best loss: 0.24236111111111114]\n",
            " 97%|█████████▋| 29/30 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 30/30 [00:05<00:00,  5.71s/trial, best loss: 0.24236111111111114]\n",
            " 97%|█████████▋| 30/31 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 31/31 [00:26<00:00, 26.63s/trial, best loss: 0.24236111111111114]\n",
            " 97%|█████████▋| 31/32 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 32/32 [00:05<00:00,  5.96s/trial, best loss: 0.24236111111111114]\n",
            " 97%|█████████▋| 32/33 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 33/33 [00:45<00:00, 45.46s/trial, best loss: 0.2188888888888889]\n",
            "100%|██████████| 34/34 [00:39<00:00, 39.89s/trial, best loss: 0.2188888888888889]\n",
            " 97%|█████████▋| 34/35 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 35/35 [00:28<00:00, 28.98s/trial, best loss: 0.2188888888888889]\n",
            " 97%|█████████▋| 35/36 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 36/36 [00:24<00:00, 24.28s/trial, best loss: 0.2188888888888889]\n",
            "100%|██████████| 37/37 [00:20<00:00, 20.23s/trial, best loss: 0.2188888888888889]\n",
            " 97%|█████████▋| 37/38 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 38/38 [00:02<00:00,  2.10s/trial, best loss: 0.2188888888888889]\n",
            "100%|██████████| 39/39 [00:11<00:00, 11.20s/trial, best loss: 0.2188888888888889]\n",
            " 98%|█████████▊| 39/40 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00,  1.75trial/s, best loss: 0.2188888888888889]\n",
            " 98%|█████████▊| 40/41 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 41/41 [00:22<00:00, 22.37s/trial, best loss: 0.2188888888888889]\n",
            "100%|██████████| 42/42 [00:42<00:00, 42.98s/trial, best loss: 0.2188888888888889]\n",
            " 98%|█████████▊| 42/43 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 43/43 [00:09<00:00,  9.98s/trial, best loss: 0.2188888888888889]\n",
            "100%|██████████| 44/44 [00:16<00:00, 16.98s/trial, best loss: 0.2188888888888889]\n",
            " 98%|█████████▊| 44/45 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 45/45 [00:36<00:00, 36.09s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 45/46 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 46/46 [00:34<00:00, 34.03s/trial, best loss: 0.21625000000000005]\n",
            "100%|██████████| 47/47 [00:06<00:00,  6.77s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 47/48 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 48/48 [00:14<00:00, 14.41s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 48/49 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 49/49 [00:08<00:00,  8.59s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 49/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 50/50 [00:02<00:00,  2.12s/trial, best loss: 0.21625000000000005]\n",
            "100%|██████████| 51/51 [00:18<00:00, 18.03s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 51/52 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00, 12.05s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 52/53 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 53/53 [00:27<00:00, 27.09s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 53/54 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 54/54 [00:38<00:00, 38.56s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 54/55 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 55/55 [00:05<00:00,  5.82s/trial, best loss: 0.21625000000000005]\n",
            "100%|██████████| 56/56 [00:09<00:00,  9.42s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 56/57 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 57/57 [00:26<00:00, 26.77s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 57/58 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 58/58 [00:03<00:00,  3.80s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 58/59 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 59/59 [00:14<00:00, 14.20s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 59/60 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 60/60 [00:01<00:00,  1.85s/trial, best loss: 0.21625000000000005]\n",
            "100%|██████████| 61/61 [00:02<00:00,  2.78s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 61/62 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 62/62 [00:42<00:00, 42.02s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 62/63 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 63/63 [00:34<00:00, 34.69s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 63/64 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 64/64 [00:08<00:00,  8.54s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 64/65 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 65/65 [00:16<00:00, 16.48s/trial, best loss: 0.21625000000000005]\n",
            " 98%|█████████▊| 65/66 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 66/66 [00:23<00:00, 23.06s/trial, best loss: 0.21625000000000005]\n",
            " 99%|█████████▊| 66/67 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 67/67 [00:26<00:00, 26.22s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 67/68 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 68/68 [00:30<00:00, 30.79s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 68/69 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 69/69 [00:39<00:00, 39.19s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 69/70 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 70/70 [00:15<00:00, 15.86s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 70/71 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 71/71 [00:36<00:00, 36.89s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 71/72 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 72/72 [00:06<00:00,  6.99s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 72/73 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 73/73 [00:25<00:00, 25.47s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 73/74 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 74/74 [00:31<00:00, 31.21s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 74/75 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 75/75 [00:13<00:00, 13.72s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 75/76 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 76/76 [00:32<00:00, 32.97s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 76/77 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 77/77 [00:17<00:00, 17.07s/trial, best loss: 0.2146527777777778]\n",
            "100%|██████████| 78/78 [00:08<00:00,  8.94s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▊| 78/79 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 79/79 [00:21<00:00, 21.19s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 79/80 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 80/80 [00:03<00:00,  3.33s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 80/81 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 81/81 [00:04<00:00,  4.77s/trial, best loss: 0.2146527777777778]\n",
            "100%|██████████| 82/82 [00:22<00:00, 22.51s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 82/83 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 83/83 [00:06<00:00,  6.62s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 83/84 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:646: RuntimeWarning: overflow encountered in exp\n",
            "  sample_weight *= np.exp(\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1351: UserWarning: Sample weights have reached infinite values, at iteration 1, causing overflow. Iterations stopped. Try lowering the learning rate.\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 84/84 [00:00<00:00,  4.40trial/s, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 84/85 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 85/85 [00:25<00:00, 25.34s/trial, best loss: 0.2146527777777778]\n",
            "100%|██████████| 86/86 [00:06<00:00,  6.65s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 86/87 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 87/87 [00:11<00:00, 11.25s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 87/88 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 88/88 [00:42<00:00, 42.54s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 88/89 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 89/89 [00:01<00:00,  1.01s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 89/90 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 90/90 [00:16<00:00, 16.26s/trial, best loss: 0.2146527777777778]\n",
            "100%|██████████| 91/91 [00:12<00:00, 12.30s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 91/92 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 92/92 [00:19<00:00, 19.99s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 92/93 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 93/93 [00:34<00:00, 34.10s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 93/94 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 94/94 [00:09<00:00,  9.24s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 94/95 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 95/95 [00:04<00:00,  4.15s/trial, best loss: 0.2146527777777778]\n",
            "100%|██████████| 96/96 [00:15<00:00, 15.76s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 96/97 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 97/97 [00:12<00:00, 12.62s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 97/98 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 98/98 [00:28<00:00, 28.40s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 98/99 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 99/99 [00:21<00:00, 21.85s/trial, best loss: 0.2146527777777778]\n",
            " 99%|█████████▉| 99/100 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00,  8.60s/trial, best loss: 0.2146527777777778]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: AdaBoostClassifier(learning_rate=1.8070483079192097, n_estimators=587,\n",
            "                   random_state=1)\n",
            "Best Score: 0.2146527777777778\n",
            "Test Set Accuracy: 0.7858333333333334\n",
            "Precision: 0.7855565371024735\n",
            "Recall: 0.7880802038329456\n",
            "F2 Score: 0.7875741741209813\n",
            "\n",
            "Classifier: 0 sklearn_GradientBoostingClassifier\n",
            "1  ccp_alpha =\n",
            "2   Literal{0.0}\n",
            "3  criterion =\n",
            "4   switch\n",
            "5     hyperopt_param\n",
            "6       Literal{my_gb.gbc_criterion}\n",
            "7       randint\n",
            "8         Literal{2}\n",
            "9     Literal{friedman_mse}\n",
            "10     Literal{squared_error}\n",
            "11  init =\n",
            "12   Literal{None}\n",
            "13  learning_rate =\n",
            "14   float\n",
            "15     hyperopt_param\n",
            "16       Literal{my_gb.gbc_learning_rate}\n",
            "17       lognormal\n",
            "18         Literal{-4.605170185988091}\n",
            "19         Literal{2.3025850929940455}\n",
            "20  loss =\n",
            "21   switch\n",
            "22     hyperopt_param\n",
            "23       Literal{my_gb.gbc_loss}\n",
            "24       randint\n",
            "25         Literal{2}\n",
            "26     Literal{log_loss}\n",
            "27     Literal{exponential}\n",
            "28  max_depth =\n",
            "29   switch\n",
            "30     hyperopt_param\n",
            "31       Literal{my_gb.gbc_max_depth}\n",
            "32       categorical\n",
            "33         pos_args\n",
            "34           Literal{0.1}\n",
            "35           Literal{0.7}\n",
            "36           Literal{0.1}\n",
            "37           Literal{0.1}\n",
            "38     Literal{2}\n",
            "39     Literal{3}\n",
            "40     Literal{4}\n",
            "41     Literal{5}\n",
            "42  max_features =\n",
            "43   switch\n",
            "44     hyperopt_param\n",
            "45       Literal{my_gb.gbc_max_features}\n",
            "46       categorical\n",
            "47         pos_args\n",
            "48           Literal{0.2}\n",
            "49           Literal{0.1}\n",
            "50           Literal{0.1}\n",
            "51           Literal{0.6}\n",
            "52     Literal{sqrt}\n",
            "53     Literal{log2}\n",
            "54     Literal{None}\n",
            "55     float\n",
            "56       hyperopt_param\n",
            "57         Literal{my_gb.gbc_max_features.frac}\n",
            "58         uniform\n",
            "59           Literal{0.0}\n",
            "60           Literal{1.0}\n",
            "61  max_leaf_nodes =\n",
            "62   switch\n",
            "63     hyperopt_param\n",
            "64       Literal{my_gb.gbc_max_leaf_nodes}\n",
            "65       categorical\n",
            "66         pos_args\n",
            "67           Literal{0.85}\n",
            "68           Literal{0.05}\n",
            "69           Literal{0.05}\n",
            "70           Literal{0.05}\n",
            "71     Literal{None}\n",
            "72     Literal{5}\n",
            "73     Literal{10}\n",
            "74     Literal{15}\n",
            "75  min_impurity_decrease =\n",
            "76   switch\n",
            "77     hyperopt_param\n",
            "78       Literal{my_gb.gbc_min_impurity_decrease}\n",
            "79       categorical\n",
            "80         pos_args\n",
            "81           Literal{0.85}\n",
            "82           Literal{0.05}\n",
            "83           Literal{0.05}\n",
            "84           Literal{0.05}\n",
            "85     Literal{0.0}\n",
            "86     Literal{0.01}\n",
            "87     Literal{0.02}\n",
            "88     Literal{0.05}\n",
            "89  min_samples_leaf =\n",
            "90   switch\n",
            "91     hyperopt_param\n",
            "92       Literal{my_gb.gbc_min_samples_leaf}\n",
            "93       randint\n",
            "94         Literal{2}\n",
            "95     Literal{1}\n",
            "96     int\n",
            "97       float\n",
            "98         hyperopt_param\n",
            "99           Literal{my_gb.gbc_min_samples_leaf.gt1}\n",
            "100           qloguniform\n",
            "101             Literal{0.4054651081081644}\n",
            "102             Literal{3.921973336281314}\n",
            "103             Literal{1}\n",
            "104  min_samples_split =\n",
            "105   switch\n",
            "106     hyperopt_param\n",
            "107       Literal{my_gb.gbc_min_samples_split}\n",
            "108       categorical\n",
            "109         pos_args\n",
            "110           Literal{0.95}\n",
            "111           Literal{0.05}\n",
            "112     Literal{2}\n",
            "113     Literal{3}\n",
            "114  min_weight_fraction_leaf =\n",
            "115   Literal{0.0}\n",
            "116  n_estimators =\n",
            "117   int\n",
            "118     float\n",
            "119       hyperopt_param\n",
            "120         Literal{my_gb.gbc_n_estimators}\n",
            "121         qloguniform\n",
            "122           Literal{2.3513752571634776}\n",
            "123           Literal{6.908255154023788}\n",
            "124           Literal{1}\n",
            "125  n_iter_no_change =\n",
            "126   Literal{None}\n",
            "127  random_state =\n",
            "128   hyperopt_param\n",
            "129     Literal{my_gb.gbc_random_state}\n",
            "130     randint\n",
            "131       Literal{5}\n",
            "132  subsample =\n",
            "133   Literal{1.0}\n",
            "134  tol =\n",
            "135   Literal{0.0001}\n",
            "136  validation_fraction =\n",
            "137   Literal{0.1}\n",
            "138  verbose =\n",
            "139   Literal{False}\n",
            "140  warm_start =\n",
            "141   Literal{False}\n",
            "100%|██████████| 1/1 [01:45<00:00, 105.20s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.14s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 3/3 [00:00<00:00,  1.26trial/s, best loss: 0.2090277777777778]\n",
            "100%|██████████| 4/4 [00:03<00:00,  3.71s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 5/5 [00:28<00:00, 28.12s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 6/6 [00:19<00:00, 19.75s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 7/7 [00:06<00:00,  6.76s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 8/8 [00:09<00:00,  9.19s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 9/9 [00:01<00:00,  1.15s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 10/10 [00:00<00:00,  1.07trial/s, best loss: 0.2090277777777778]\n",
            "100%|██████████| 11/11 [00:01<00:00,  1.44s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 12/12 [00:09<00:00,  9.81s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 13/13 [00:01<00:00,  1.23s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 14/14 [00:08<00:00,  8.08s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 15/15 [00:01<00:00,  1.26s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 16/16 [01:13<00:00, 73.45s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 17/17 [00:03<00:00,  3.18s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 18/18 [00:05<00:00,  5.66s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 19/19 [00:32<00:00, 32.78s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 20/20 [00:01<00:00,  1.74s/trial, best loss: 0.2090277777777778]\n",
            "100%|██████████| 21/21 [00:26<00:00, 26.38s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 22/22 [00:22<00:00, 22.26s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 23/23 [00:13<00:00, 13.24s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 24/24 [00:41<00:00, 41.54s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 25/25 [00:38<00:00, 38.28s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 26/26 [00:07<00:00,  7.75s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 27/27 [00:20<00:00, 20.40s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 28/28 [00:48<00:00, 48.96s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 29/29 [00:05<00:00,  5.66s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 30/30 [00:04<00:00,  4.35s/trial, best loss: 0.12520833333333337]\n",
            "100%|██████████| 31/31 [00:32<00:00, 32.30s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 32/32 [00:23<00:00, 23.47s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 33/33 [00:32<00:00, 32.22s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 34/34 [00:19<00:00, 19.39s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 35/35 [00:20<00:00, 20.49s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 36/36 [00:17<00:00, 17.49s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 37/37 [00:06<00:00,  6.64s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 38/38 [00:07<00:00,  7.20s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 39/39 [00:09<00:00,  9.80s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 40/40 [00:04<00:00,  4.43s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 41/41 [00:12<00:00, 12.88s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 42/42 [00:12<00:00, 12.32s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 43/43 [00:20<00:00, 20.31s/trial, best loss: 0.08944444444444444]\n",
            "100%|██████████| 44/44 [01:03<00:00, 63.14s/trial, best loss: 0.07777777777777772]\n",
            "100%|██████████| 45/45 [00:47<00:00, 47.67s/trial, best loss: 0.07777777777777772]\n",
            "100%|██████████| 46/46 [00:00<00:00,  1.29trial/s, best loss: 0.07777777777777772]\n",
            "100%|██████████| 47/47 [01:59<00:00, 119.10s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 48/48 [01:13<00:00, 73.18s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 49/49 [00:39<00:00, 39.20s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 50/50 [01:21<00:00, 81.57s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 51/51 [00:08<00:00,  8.65s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 52/52 [00:44<00:00, 44.34s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 53/53 [00:29<00:00, 29.17s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 54/54 [00:28<00:00, 28.15s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 55/55 [00:35<00:00, 35.96s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 56/56 [00:18<00:00, 18.36s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 57/57 [00:36<00:00, 36.44s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 58/58 [00:12<00:00, 12.90s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 59/59 [00:03<00:00,  3.05s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 60/60 [00:06<00:00,  6.30s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 61/61 [00:38<00:00, 38.33s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 62/62 [00:09<00:00,  9.70s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 63/63 [00:02<00:00,  2.06s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 64/64 [00:01<00:00,  1.80s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 65/65 [00:04<00:00,  4.77s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 66/66 [00:38<00:00, 38.06s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 67/67 [00:35<00:00, 35.31s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 68/68 [00:18<00:00, 18.75s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 69/69 [00:21<00:00, 21.54s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 70/70 [00:39<00:00, 39.34s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 71/71 [00:48<00:00, 48.52s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 72/72 [00:30<00:00, 30.80s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 73/73 [00:55<00:00, 55.69s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 74/74 [00:26<00:00, 26.88s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 75/75 [00:57<00:00, 57.06s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 76/76 [00:15<00:00, 15.45s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 77/77 [00:11<00:00, 11.15s/trial, best loss: 0.050138888888888844]\n",
            "100%|██████████| 78/78 [00:54<00:00, 54.22s/trial, best loss: 0.04812499999999997]\n",
            "100%|██████████| 79/79 [00:13<00:00, 13.69s/trial, best loss: 0.04812499999999997]\n",
            "100%|██████████| 80/80 [00:00<00:00,  1.08trial/s, best loss: 0.04812499999999997]\n",
            "100%|██████████| 81/81 [00:42<00:00, 42.89s/trial, best loss: 0.04812499999999997]\n",
            "100%|██████████| 82/82 [00:47<00:00, 47.11s/trial, best loss: 0.04812499999999997]\n",
            " 99%|█████████▉| 82/83 [00:00<?, ?trial/s, best loss=?]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
        "from hyperopt import tpe\n",
        "from hpsklearn import HyperoptEstimator,any_preprocessing,random_forest_classifier,extra_tree_classifier,bagging_classifier,ada_boost_classifier,gradient_boosting_classifier,hist_gradient_boosting_classifier,ridge_classifier_cv,perceptron,decision_tree_classifier,k_neighbors_classifier,xgboost_classification\n",
        "\n",
        "# Define classifiers\n",
        "hypertopt_classification_estimators = [\n",
        "    #random_forest_classifier('my_rf'),\n",
        "    #extra_tree_classifier('my_et'),\n",
        "    #bagging_classifier('my_bag'),\n",
        "    #ada_boost_classifier('my_ada'),\n",
        "    #gradient_boosting_classifier('my_gb'),\n",
        "    #hist_gradient_boosting_classifier('my_hgb'),\n",
        "    #ridge_classifier_cv('my_ridge'),\n",
        "    perceptron('my_perceptron'),\n",
        "    #decision_tree_classifier('my_dt'),\n",
        "    k_neighbors_classifier('my_knn'),\n",
        "    xgboost_classification('my_xgb'),\n",
        "]\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for classifier in hypertopt_classification_estimators:\n",
        "    print(f\"\\nClassifier: {classifier}\")\n",
        "\n",
        "    # Create HyperoptEstimator\n",
        "    estim = HyperoptEstimator(\n",
        "        classifier=classifier,\n",
        "        preprocessing=any_preprocessing('my_pre'),\n",
        "        algo=tpe.suggest,\n",
        "        max_evals=100,\n",
        "        trial_timeout=120,\n",
        "    )\n",
        "\n",
        "    # Fit the estimator\n",
        "    estim.fit(x_std_train, y_train)\n",
        "\n",
        "    # Get the best parameters and score\n",
        "    best_params = estim._best_learner\n",
        "    best_score = estim._best_loss\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = estim.predict(x_std_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
        "\n",
        "    # Store results in the dictionary\n",
        "    results[classifier] = {\n",
        "        'best_params': best_params,\n",
        "        'best_score': best_score,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f2_score': f2_score,\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    print(f\"Best Score: {best_score}\")\n",
        "    print(f\"Test Set Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F2 Score: {f2_score}\")\n",
        "\n",
        "# Find the best model based on test accuracy\n",
        "best_model = max(results, key=lambda x: results[x]['accuracy'])\n",
        "\n",
        "# Print overall best model\n",
        "print(f\"\\nBest Model Overall: {best_model}\")\n"
      ],
      "metadata": {
        "id": "54PbALbyE-0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd53a37-e1f1-49ea-8285-aba21351bda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifier: 0 sklearn_Perceptron\n",
            "1  alpha =\n",
            "2   float\n",
            "3     hyperopt_param\n",
            "4       Literal{my_perceptron.perceptron_alpha}\n",
            "5       loguniform\n",
            "6         Literal{-13.815510557964274}\n",
            "7         Literal{-2.3025850929940455}\n",
            "8  class_weight =\n",
            "9   Literal{None}\n",
            "10  early_stopping =\n",
            "11   Literal{False}\n",
            "12  eta0 =\n",
            "13   float\n",
            "14     hyperopt_param\n",
            "15       Literal{my_perceptron.perceptron_eta0}\n",
            "16       normal\n",
            "17        mu =\n",
            "18         Literal{1.0}\n",
            "19        sigma =\n",
            "20         Literal{0.1}\n",
            "21  fit_intercept =\n",
            "22   Literal{True}\n",
            "23  l1_ratio =\n",
            "24   float\n",
            "25     hyperopt_param\n",
            "26       Literal{my_perceptron.perceptron_l1_ratio}\n",
            "27       loguniform\n",
            "28         Literal{-16.11809565095832}\n",
            "29         Literal{0.0}\n",
            "30  max_iter =\n",
            "31   int\n",
            "32     float\n",
            "33       hyperopt_param\n",
            "34         Literal{my_perceptron.perceptron_max_iter}\n",
            "35         uniform\n",
            "36           Literal{750}\n",
            "37           Literal{1250}\n",
            "38  n_iter_no_change =\n",
            "39   Literal{5}\n",
            "40  n_jobs =\n",
            "41   Literal{1}\n",
            "42  penalty =\n",
            "43   switch\n",
            "44     hyperopt_param\n",
            "45       Literal{my_perceptron.perceptron_penalty}\n",
            "46       randint\n",
            "47         Literal{3}\n",
            "48     Literal{l1}\n",
            "49     Literal{l2}\n",
            "50     Literal{elasticnet}\n",
            "51  random_state =\n",
            "52   hyperopt_param\n",
            "53     Literal{my_perceptron.perceptron_random_state}\n",
            "54     randint\n",
            "55       Literal{5}\n",
            "56  shuffle =\n",
            "57   Literal{True}\n",
            "58  tol =\n",
            "59   float\n",
            "60     hyperopt_param\n",
            "61       Literal{my_perceptron.perceptron_tol}\n",
            "62       loguniform\n",
            "63         Literal{-11.512925464970229}\n",
            "64         Literal{-4.605170185988091}\n",
            "65  validation_fraction =\n",
            "66   Literal{0.1}\n",
            "67  verbose =\n",
            "68   Literal{0}\n",
            "69  warm_start =\n",
            "70   Literal{False}\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.77trial/s, best loss: 0.4184027777777778]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.97trial/s, best loss: 0.4184027777777778]\n",
            "100%|██████████| 3/3 [00:00<00:00,  5.76trial/s, best loss: 0.3970138888888889]\n",
            "100%|██████████| 4/4 [00:00<00:00,  5.07trial/s, best loss: 0.3970138888888889]\n",
            "100%|██████████| 5/5 [00:00<00:00,  3.90trial/s, best loss: 0.3375694444444445]\n",
            "100%|██████████| 6/6 [00:00<00:00,  3.68trial/s, best loss: 0.3375694444444445]\n",
            "100%|██████████| 7/7 [00:00<00:00,  6.13trial/s, best loss: 0.3375694444444445]\n",
            "100%|██████████| 8/8 [00:00<00:00,  9.05trial/s, best loss: 0.3375694444444445]\n",
            "100%|██████████| 9/9 [00:00<00:00,  9.31trial/s, best loss: 0.3375694444444445]\n",
            "100%|██████████| 10/10 [00:00<00:00,  8.97trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 11/11 [00:00<00:00,  8.92trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 12/12 [00:00<00:00,  5.84trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 13/13 [00:00<00:00,  3.87trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 14/14 [00:00<00:00,  4.49trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 15/15 [00:00<00:00,  7.88trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 16/16 [00:00<00:00, 10.19trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 17/17 [00:00<00:00,  2.60trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 18/18 [00:00<00:00,  7.49trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 19/19 [00:00<00:00,  8.84trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 20/20 [00:00<00:00,  6.95trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 21/21 [00:00<00:00,  6.79trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 22/22 [00:00<00:00,  5.81trial/s, best loss: 0.3346527777777778]\n",
            "100%|██████████| 23/23 [00:00<00:00,  3.98trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 24/24 [00:00<00:00,  7.95trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 25/25 [00:00<00:00,  8.72trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 26/26 [00:00<00:00,  7.53trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 27/27 [00:00<00:00,  8.76trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 28/28 [00:00<00:00,  7.81trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 29/29 [00:00<00:00,  8.04trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 30/30 [00:00<00:00,  8.12trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 31/31 [00:00<00:00,  8.35trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 32/32 [00:00<00:00,  8.32trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 33/33 [00:00<00:00,  9.78trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 34/34 [00:00<00:00,  6.83trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 35/35 [00:00<00:00,  7.62trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 36/36 [00:00<00:00,  8.50trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 37/37 [00:00<00:00,  6.36trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 38/38 [00:00<00:00,  5.22trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 39/39 [00:00<00:00,  8.46trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 40/40 [00:00<00:00,  8.07trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 41/41 [00:00<00:00,  7.87trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 42/42 [00:00<00:00,  3.32trial/s, best loss: 0.3208333333333333]\n",
            "100%|██████████| 43/43 [00:00<00:00,  6.74trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 44/44 [00:00<00:00,  5.63trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 45/45 [00:00<00:00,  5.89trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 46/46 [00:00<00:00,  4.89trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 47/47 [00:00<00:00,  5.98trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 48/48 [00:00<00:00,  6.97trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 49/49 [00:00<00:00,  7.77trial/s, best loss: 0.3153472222222222]\n",
            "100%|██████████| 50/50 [00:00<00:00,  3.93trial/s, best loss: 0.3091666666666667]\n",
            "100%|██████████| 51/51 [00:00<00:00,  3.68trial/s, best loss: 0.3091666666666667]\n",
            "100%|██████████| 52/52 [00:00<00:00,  3.41trial/s, best loss: 0.3091666666666667]\n",
            "100%|██████████| 53/53 [00:00<00:00,  3.04trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 54/54 [00:00<00:00,  2.71trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 55/55 [00:00<00:00,  7.29trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 56/56 [00:00<00:00,  8.16trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 57/57 [00:00<00:00,  7.86trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 58/58 [00:00<00:00,  6.25trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 59/59 [00:00<00:00,  5.31trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 60/60 [00:00<00:00,  3.84trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 61/61 [00:00<00:00,  5.68trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 62/62 [00:00<00:00,  3.22trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 63/63 [00:00<00:00,  4.85trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 64/64 [00:00<00:00,  2.76trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 65/65 [00:00<00:00,  3.99trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 66/66 [00:00<00:00,  1.49trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 67/67 [00:00<00:00,  2.56trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 68/68 [00:00<00:00,  3.37trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 69/69 [00:00<00:00,  7.52trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 70/70 [00:00<00:00,  6.50trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 71/71 [00:00<00:00,  7.37trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 72/72 [00:00<00:00,  6.79trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 73/73 [00:00<00:00,  7.35trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 74/74 [00:00<00:00,  6.57trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 75/75 [00:00<00:00,  3.24trial/s, best loss: 0.30819444444444444]\n",
            "100%|██████████| 76/76 [00:00<00:00,  4.88trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 77/77 [00:00<00:00,  4.77trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 78/78 [00:00<00:00,  6.62trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 79/79 [00:00<00:00,  3.07trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 80/80 [00:00<00:00,  3.11trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 81/81 [00:00<00:00,  6.19trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 82/82 [00:00<00:00,  4.66trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 83/83 [00:00<00:00,  8.20trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 84/84 [00:00<00:00,  5.86trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 85/85 [00:00<00:00,  4.18trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 86/86 [00:00<00:00,  8.13trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 87/87 [00:00<00:00,  5.00trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 88/88 [00:00<00:00,  8.26trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 89/89 [00:00<00:00,  8.58trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 90/90 [00:00<00:00,  7.01trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 91/91 [00:00<00:00,  3.30trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 92/92 [00:00<00:00,  4.52trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 93/93 [00:00<00:00,  8.07trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 94/94 [00:00<00:00,  6.07trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 95/95 [00:00<00:00,  5.85trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 96/96 [00:00<00:00,  7.65trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 97/97 [00:00<00:00,  3.23trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 98/98 [00:00<00:00,  7.81trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 99/99 [00:00<00:00,  5.44trial/s, best loss: 0.2850694444444445]\n",
            "100%|██████████| 100/100 [00:00<00:00,  6.17trial/s, best loss: 0.2850694444444445]\n",
            "Best Parameters: Perceptron(alpha=0.006613162498796316, eta0=1.1335444727408035,\n",
            "           l1_ratio=0.00018003190654967917, max_iter=1152, n_jobs=1,\n",
            "           penalty='elasticnet', random_state=1, tol=0.002463031022829648)\n",
            "Best Score: 0.2850694444444445\n",
            "Test Set Accuracy: 0.523\n",
            "Precision: 0.5182008766914428\n",
            "Recall: 0.606378233719893\n",
            "F2 Score: 0.5864210844153043\n",
            "\n",
            "Classifier: 0 sklearn_KNeighborsClassifier\n",
            "1  algorithm =\n",
            "2   switch\n",
            "3     hyperopt_param\n",
            "4       Literal{my_knn.k_neighbors_classifier_algorithm}\n",
            "5       randint\n",
            "6         Literal{4}\n",
            "7     Literal{auto}\n",
            "8     Literal{ball_tree}\n",
            "9     Literal{kd_tree}\n",
            "10     Literal{brute}\n",
            "11  leaf_size =\n",
            "12   int\n",
            "13     float\n",
            "14       hyperopt_param\n",
            "15         Literal{my_knn.k_neighbors_classifier_leaf_size}\n",
            "16         uniform\n",
            "17           Literal{20}\n",
            "18           Literal{40}\n",
            "19  metric =\n",
            "20   switch\n",
            "21     hyperopt_param\n",
            "22       Literal{my_knn.k_neighbors_classifier_metric}\n",
            "23       randint\n",
            "24         Literal{6}\n",
            "25     Literal{cityblock}\n",
            "26     Literal{l1}\n",
            "27     Literal{l2}\n",
            "28     Literal{minkowski}\n",
            "29     Literal{euclidean}\n",
            "30     Literal{manhattan}\n",
            "31  metric_params =\n",
            "32   Literal{None}\n",
            "33  n_jobs =\n",
            "34   Literal{1}\n",
            "35  n_neighbors =\n",
            "36   int\n",
            "37     float\n",
            "38       hyperopt_param\n",
            "39         Literal{my_knn.k_neighbors_classifier_n_neighbors}\n",
            "40         uniform\n",
            "41           Literal{1}\n",
            "42           Literal{15}\n",
            "43  p =\n",
            "44   float\n",
            "45     hyperopt_param\n",
            "46       Literal{my_knn.k_neighbors_classifier_p}\n",
            "47       uniform\n",
            "48         Literal{1}\n",
            "49         Literal{5}\n",
            "50  weights =\n",
            "51   switch\n",
            "52     hyperopt_param\n",
            "53       Literal{my_knn.k_neighbors_classifier_weights}\n",
            "54       randint\n",
            "55         Literal{2}\n",
            "56     Literal{uniform}\n",
            "57     Literal{distance}\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.11trial/s, best loss: 0.15951388888888884]\n",
            "100%|██████████| 2/2 [00:00<00:00,  1.01trial/s, best loss: 0.15583333333333338]\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.12s/trial, best loss: 0.0814583333333333]\n",
            "100%|██████████| 4/4 [00:11<00:00, 11.71s/trial, best loss: 0.0814583333333333]\n",
            "100%|██████████| 5/5 [00:01<00:00,  1.93s/trial, best loss: 0.0814583333333333]\n",
            "100%|██████████| 6/6 [00:01<00:00,  1.88s/trial, best loss: 0.05138888888888893]\n",
            "100%|██████████| 7/7 [00:00<00:00,  1.02trial/s, best loss: 0.05138888888888893]\n",
            "100%|██████████| 8/8 [00:00<00:00,  1.18trial/s, best loss: 0.05138888888888893]\n",
            "100%|██████████| 9/9 [00:01<00:00,  1.22s/trial, best loss: 0.05138888888888893]\n",
            "100%|██████████| 10/10 [00:08<00:00,  8.25s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 11/11 [00:00<00:00,  1.20trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 12/12 [00:01<00:00,  1.30s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 13/13 [00:00<00:00,  1.01trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 14/14 [00:10<00:00, 10.04s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 15/15 [00:00<00:00,  1.03trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 16/16 [00:10<00:00, 10.27s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 17/17 [00:01<00:00,  1.17s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 18/18 [00:00<00:00,  1.13trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 19/19 [00:02<00:00,  2.37s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 20/20 [00:00<00:00,  1.15trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 21/21 [00:00<00:00,  1.35trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 22/22 [00:32<00:00, 32.87s/trial, best loss: 0.04618055555555556]\n",
            "100%|██████████| 23/23 [00:00<00:00,  1.13trial/s, best loss: 0.04618055555555556]\n",
            "100%|██████████| 24/24 [00:04<00:00,  4.47s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 25/25 [00:07<00:00,  7.84s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 26/26 [00:07<00:00,  7.98s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 27/27 [00:06<00:00,  6.41s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 28/28 [00:05<00:00,  5.18s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 29/29 [00:21<00:00, 21.50s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 30/30 [02:00<00:00, 120.15s/trial, best loss: 0.045486111111111116]\n",
            "100%|██████████| 31/31 [00:06<00:00,  6.19s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 32/32 [00:04<00:00,  4.11s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 33/33 [02:00<00:00, 120.14s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 34/34 [00:08<00:00,  8.68s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 35/35 [00:01<00:00,  1.30s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 36/36 [00:01<00:00,  1.25s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 37/37 [00:00<00:00,  3.17trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 38/38 [02:00<00:00, 120.18s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 39/39 [00:00<00:00,  1.49trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 40/40 [00:05<00:00,  5.32s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 41/41 [00:00<00:00,  1.58trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 42/42 [00:00<00:00,  4.64trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 43/43 [00:00<00:00,  4.15trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 44/44 [00:02<00:00,  2.96s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 45/45 [00:04<00:00,  4.86s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 46/46 [00:01<00:00,  1.83s/trial, best loss: 0.04541666666666666]\n",
            "100%|██████████| 47/47 [00:00<00:00,  4.40trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 48/48 [00:00<00:00,  1.06trial/s, best loss: 0.04541666666666666]\n",
            "100%|██████████| 49/49 [00:00<00:00,  4.61trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 50/50 [00:00<00:00,  1.14trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 51/51 [00:00<00:00,  2.76trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 52/52 [00:00<00:00,  2.39trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 53/53 [00:00<00:00,  1.10trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 54/54 [00:00<00:00,  3.40trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 55/55 [00:00<00:00,  3.28trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 56/56 [00:00<00:00,  1.07trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 57/57 [00:00<00:00,  3.11trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 58/58 [00:10<00:00, 10.18s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 59/59 [00:00<00:00,  1.11trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 60/60 [00:00<00:00,  2.66trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 61/61 [00:12<00:00, 12.18s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 62/62 [00:00<00:00,  2.13trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 63/63 [00:01<00:00,  1.79s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 64/64 [00:16<00:00, 16.00s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 65/65 [00:00<00:00,  3.06trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 66/66 [00:00<00:00,  3.19trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 67/67 [00:00<00:00,  3.49trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 68/68 [00:00<00:00,  2.64trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 69/69 [00:00<00:00,  4.27trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 70/70 [00:00<00:00,  3.62trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 71/71 [00:00<00:00,  2.91trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 72/72 [00:00<00:00,  2.66trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 73/73 [00:03<00:00,  3.15s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 74/74 [00:00<00:00,  1.59trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 75/75 [00:07<00:00,  7.67s/trial, best loss: 0.04326388888888888]\n",
            "100%|██████████| 76/76 [00:00<00:00,  2.16trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 77/77 [00:00<00:00,  2.11trial/s, best loss: 0.04326388888888888]\n",
            "100%|██████████| 78/78 [00:00<00:00,  2.83trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 79/79 [00:00<00:00,  1.84trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 80/80 [00:00<00:00,  1.40trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 81/81 [00:01<00:00,  1.79s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 82/82 [00:01<00:00,  1.02s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 83/83 [00:00<00:00,  1.29trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 84/84 [00:00<00:00,  1.44trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 85/85 [00:03<00:00,  3.41s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 86/86 [00:12<00:00, 12.96s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 87/87 [00:00<00:00,  2.21trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 88/88 [00:00<00:00,  1.82trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 89/89 [00:00<00:00,  3.24trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 90/90 [00:01<00:00,  1.45s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 91/91 [00:00<00:00,  1.59trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 92/92 [00:00<00:00,  2.48trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 93/93 [00:00<00:00,  1.66trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 94/94 [00:01<00:00,  1.50s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 95/95 [00:00<00:00,  1.37trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 96/96 [00:00<00:00,  2.63trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 97/97 [00:15<00:00, 15.44s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 98/98 [00:00<00:00,  1.82trial/s, best loss: 0.043194444444444424]\n",
            "100%|██████████| 99/99 [00:14<00:00, 14.47s/trial, best loss: 0.043194444444444424]\n",
            "100%|██████████| 100/100 [00:00<00:00,  1.62trial/s, best loss: 0.043194444444444424]\n",
            "Best Parameters: KNeighborsClassifier(algorithm='kd_tree', leaf_size=22, n_jobs=1, n_neighbors=2,\n",
            "                     p=2.372428226692859, weights='distance')\n",
            "Best Score: 0.043194444444444424\n",
            "Test Set Accuracy: 0.9585\n",
            "Precision: 0.9891705343329763\n",
            "Recall: 0.9268510258697591\n",
            "F2 Score: 0.9386787125917561\n",
            "\n",
            "Classifier: 0 sklearn_XGBClassifier\n",
            "1  base_score =\n",
            "2   Literal{0.5}\n",
            "3  colsample_bylevel =\n",
            "4   float\n",
            "5     hyperopt_param\n",
            "6       Literal{my_xgb.xgboost_clf_colsample_bylevel}\n",
            "7       uniform\n",
            "8         Literal{0.5}\n",
            "9         Literal{1}\n",
            "10  colsample_bytree =\n",
            "11   float\n",
            "12     hyperopt_param\n",
            "13       Literal{my_xgb.xgboost_clf_colsample_bytree}\n",
            "14       uniform\n",
            "15         Literal{0.5}\n",
            "16         Literal{1}\n",
            "17  gamma =\n",
            "18   sub\n",
            "19     float\n",
            "20       hyperopt_param\n",
            "21         Literal{my_xgb.xgboost_clf_gamma}\n",
            "22         loguniform\n",
            "23           Literal{-9.210340371976182}\n",
            "24           Literal{1.6094379124341003}\n",
            "25     Literal{0.0001}\n",
            "26  learning_rate =\n",
            "27   sub\n",
            "28     float\n",
            "29       hyperopt_param\n",
            "30         Literal{my_xgb.xgboost_clf_learning_rate}\n",
            "31         loguniform\n",
            "32           Literal{-9.210340371976182}\n",
            "33           Literal{-0.6931471805599453}\n",
            "34     Literal{0.0001}\n",
            "35  max_delta_step =\n",
            "36   Literal{0}\n",
            "37  max_depth =\n",
            "38   int\n",
            "39     float\n",
            "40       hyperopt_param\n",
            "41         Literal{my_xgb.xgboost_clf_max_depth}\n",
            "42         uniform\n",
            "43           Literal{1}\n",
            "44           Literal{11}\n",
            "45  min_child_weight =\n",
            "46   int\n",
            "47     float\n",
            "48       hyperopt_param\n",
            "49         Literal{my_xgb.xgboost_clf_min_child_weight}\n",
            "50         loguniform\n",
            "51           Literal{0.0}\n",
            "52           Literal{4.605170185988091}\n",
            "53  n_estimators =\n",
            "54   int\n",
            "55     float\n",
            "56       hyperopt_param\n",
            "57         Literal{my_xgb.xgboost_clf_n_estimators}\n",
            "58         quniform\n",
            "59           Literal{100}\n",
            "60           Literal{6000}\n",
            "61           Literal{200}\n",
            "62  n_jobs =\n",
            "63   Literal{-1}\n",
            "64  objective =\n",
            "65   Literal{binary:logistic}\n",
            "66  reg_alpha =\n",
            "67   sub\n",
            "68     float\n",
            "69       hyperopt_param\n",
            "70         Literal{my_xgb.xgboost_clf_reg_alpha}\n",
            "71         loguniform\n",
            "72           Literal{-9.210340371976182}\n",
            "73           Literal{0.0}\n",
            "74     Literal{0.0001}\n",
            "75  reg_lambda =\n",
            "76   float\n",
            "77     hyperopt_param\n",
            "78       Literal{my_xgb.xgboost_clf_reg_lambda}\n",
            "79       loguniform\n",
            "80         Literal{0.0}\n",
            "81         Literal{1.3862943611198906}\n",
            "82  scale_pos_weight =\n",
            "83   Literal{1}\n",
            "84  seed =\n",
            "85   hyperopt_param\n",
            "86     Literal{my_xgb.xgboost_clf_random_state}\n",
            "87     randint\n",
            "88       Literal{5}\n",
            "89  subsample =\n",
            "90   float\n",
            "91     hyperopt_param\n",
            "92       Literal{my_xgb.xgboost_clf_subsample}\n",
            "93       uniform\n",
            "94         Literal{0.5}\n",
            "95         Literal{1}\n",
            "96  use_label_encoder =\n",
            "97   Literal{False}\n",
            "100%|██████████| 1/1 [00:24<00:00, 24.06s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 2/2 [00:07<00:00,  7.35s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 3/3 [00:39<00:00, 39.39s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 4/4 [00:11<00:00, 11.85s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 5/5 [00:21<00:00, 21.76s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 6/6 [00:07<00:00,  7.91s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 7/7 [00:00<00:00,  1.38trial/s, best loss: 0.08423611111111107]\n",
            "100%|██████████| 8/8 [00:06<00:00,  6.71s/trial, best loss: 0.08423611111111107]\n",
            "100%|██████████| 9/9 [00:26<00:00, 26.64s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 10/10 [00:02<00:00,  2.21s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 11/11 [00:49<00:00, 49.34s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 12/12 [00:02<00:00,  2.31s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 13/13 [00:00<00:00,  1.52trial/s, best loss: 0.042916666666666714]\n",
            "100%|██████████| 14/14 [00:09<00:00,  9.75s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 15/15 [00:17<00:00, 17.37s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 16/16 [00:15<00:00, 15.17s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 17/17 [00:01<00:00,  1.24s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 18/18 [00:28<00:00, 28.86s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 19/19 [00:05<00:00,  5.98s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 20/20 [00:08<00:00,  8.47s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 21/21 [00:42<00:00, 42.08s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 22/22 [00:38<00:00, 38.70s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 23/23 [00:18<00:00, 18.59s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 24/24 [00:42<00:00, 42.47s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 25/25 [00:20<00:00, 20.32s/trial, best loss: 0.042916666666666714]\n",
            "100%|██████████| 26/26 [00:41<00:00, 41.43s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 27/27 [00:06<00:00,  6.72s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 28/28 [00:26<00:00, 26.13s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 29/29 [00:17<00:00, 17.36s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 30/30 [00:32<00:00, 32.87s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 31/31 [00:06<00:00,  6.47s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 32/32 [00:09<00:00,  9.27s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 33/33 [00:31<00:00, 31.81s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 34/34 [00:25<00:00, 25.72s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 35/35 [00:20<00:00, 20.77s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 36/36 [00:20<00:00, 20.68s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 37/37 [00:10<00:00, 10.77s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 38/38 [00:19<00:00, 19.02s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 39/39 [00:30<00:00, 30.27s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 40/40 [00:04<00:00,  4.27s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 41/41 [00:07<00:00,  7.33s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 42/42 [00:39<00:00, 39.98s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 43/43 [00:11<00:00, 11.70s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 44/44 [00:09<00:00,  9.59s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 45/45 [00:43<00:00, 43.35s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 46/46 [00:27<00:00, 27.16s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 47/47 [00:04<00:00,  4.04s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 48/48 [00:44<00:00, 44.85s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 49/49 [00:13<00:00, 13.89s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 50/50 [00:15<00:00, 15.93s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 51/51 [00:25<00:00, 25.27s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 52/52 [00:06<00:00,  6.91s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 53/53 [00:44<00:00, 44.25s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 54/54 [00:06<00:00,  6.82s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 55/55 [00:13<00:00, 13.61s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 56/56 [00:17<00:00, 17.01s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 57/57 [00:10<00:00, 10.61s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 58/58 [00:17<00:00, 17.37s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 59/59 [00:10<00:00, 10.91s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 60/60 [00:03<00:00,  3.40s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 61/61 [00:17<00:00, 17.11s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 62/62 [00:19<00:00, 19.87s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 63/63 [00:10<00:00, 10.83s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 64/64 [00:37<00:00, 37.72s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 65/65 [00:19<00:00, 19.98s/trial, best loss: 0.042013888888888906]\n",
            "100%|██████████| 66/66 [00:06<00:00,  6.34s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 67/67 [00:05<00:00,  5.08s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 68/68 [00:04<00:00,  4.83s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 69/69 [00:02<00:00,  2.77s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 70/70 [00:03<00:00,  3.99s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 71/71 [00:08<00:00,  8.45s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 72/72 [00:01<00:00,  1.35s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 73/73 [00:03<00:00,  3.03s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 74/74 [00:10<00:00, 10.08s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 75/75 [00:05<00:00,  5.12s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 76/76 [00:01<00:00,  1.42s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 77/77 [00:05<00:00,  5.58s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 78/78 [00:14<00:00, 14.66s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 79/79 [00:03<00:00,  3.17s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 80/80 [00:10<00:00, 10.72s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 81/81 [00:07<00:00,  7.01s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 82/82 [00:05<00:00,  5.54s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 83/83 [00:02<00:00,  2.20s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 84/84 [00:04<00:00,  4.34s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 85/85 [00:18<00:00, 18.12s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 86/86 [00:15<00:00, 15.36s/trial, best loss: 0.04097222222222219]\n",
            "100%|██████████| 87/87 [00:20<00:00, 20.87s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 88/88 [00:26<00:00, 26.57s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 89/89 [00:20<00:00, 20.78s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 90/90 [00:08<00:00,  8.07s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 91/91 [00:22<00:00, 22.94s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 92/92 [00:13<00:00, 13.53s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 93/93 [00:23<00:00, 23.82s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 94/94 [00:14<00:00, 14.67s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 95/95 [00:33<00:00, 33.99s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 96/96 [00:10<00:00, 10.27s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 97/97 [00:15<00:00, 15.53s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 98/98 [00:27<00:00, 27.23s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 99/99 [00:13<00:00, 13.13s/trial, best loss: 0.04069444444444448]\n",
            "100%|██████████| 100/100 [00:10<00:00, 10.56s/trial, best loss: 0.04069444444444448]\n",
            "Best Parameters: XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
            "              colsample_bylevel=0.7411992060554791, colsample_bynode=None,\n",
            "              colsample_bytree=0.6644703736569552, device=None,\n",
            "              early_stopping_rounds=None, enable_categorical=False,\n",
            "              eval_metric=None, feature_types=None, gamma=0.0295687727673445,\n",
            "              grow_policy=None, importance_type=None,\n",
            "              interaction_constraints=None, learning_rate=0.08628276003151057,\n",
            "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "              max_delta_step=0, max_depth=9, max_leaves=None,\n",
            "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
            "              multi_strategy=None, n_estimators=2800, n_jobs=1,\n",
            "              num_parallel_tree=None, random_state=None, ...)\n",
            "Best Score: 0.04069444444444448\n",
            "Test Set Accuracy: 0.9547222222222222\n",
            "Precision: 0.9836279511211294\n",
            "Recall: 0.9245093666369313\n",
            "F2 Score: 0.9357576578406808\n",
            "\n",
            "Best Model Overall: 0 sklearn_KNeighborsClassifier\n",
            "1  algorithm =\n",
            "2   switch\n",
            "3     hyperopt_param\n",
            "4       Literal{my_knn.k_neighbors_classifier_algorithm}\n",
            "5       randint\n",
            "6         Literal{4}\n",
            "7     Literal{auto}\n",
            "8     Literal{ball_tree}\n",
            "9     Literal{kd_tree}\n",
            "10     Literal{brute}\n",
            "11  leaf_size =\n",
            "12   int\n",
            "13     float\n",
            "14       hyperopt_param\n",
            "15         Literal{my_knn.k_neighbors_classifier_leaf_size}\n",
            "16         uniform\n",
            "17           Literal{20}\n",
            "18           Literal{40}\n",
            "19  metric =\n",
            "20   switch\n",
            "21     hyperopt_param\n",
            "22       Literal{my_knn.k_neighbors_classifier_metric}\n",
            "23       randint\n",
            "24         Literal{6}\n",
            "25     Literal{cityblock}\n",
            "26     Literal{l1}\n",
            "27     Literal{l2}\n",
            "28     Literal{minkowski}\n",
            "29     Literal{euclidean}\n",
            "30     Literal{manhattan}\n",
            "31  metric_params =\n",
            "32   Literal{None}\n",
            "33  n_jobs =\n",
            "34   Literal{1}\n",
            "35  n_neighbors =\n",
            "36   int\n",
            "37     float\n",
            "38       hyperopt_param\n",
            "39         Literal{my_knn.k_neighbors_classifier_n_neighbors}\n",
            "40         uniform\n",
            "41           Literal{1}\n",
            "42           Literal{15}\n",
            "43  p =\n",
            "44   float\n",
            "45     hyperopt_param\n",
            "46       Literal{my_knn.k_neighbors_classifier_p}\n",
            "47       uniform\n",
            "48         Literal{1}\n",
            "49         Literal{5}\n",
            "50  weights =\n",
            "51   switch\n",
            "52     hyperopt_param\n",
            "53       Literal{my_knn.k_neighbors_classifier_weights}\n",
            "54       randint\n",
            "55         Literal{2}\n",
            "56     Literal{uniform}\n",
            "57     Literal{distance}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(bootstrap=False, class_weight='balanced',\n",
        "                       max_features=0.2013603334303137, n_estimators=473,\n",
        "                       n_jobs=1, random_state=4, verbose=False)\n",
        "\n",
        "rf_model.fit(x_std_train, y_train)\n",
        "\n",
        "#Train details\n",
        "y_pred = rf_model.predict(x_std_train)\n",
        "accuracy = accuracy_score(y_train,y_pred)\n",
        "precision = precision_score(y_train,y_pred)\n",
        "recall = recall_score(y_train,y_pred)\n",
        "f2_scoree = fbeta_score(y_train,y_pred,beta=2)\n",
        "\n",
        "print(f'Train data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n",
        "\n",
        "#Test details\n",
        "y_pred = rf_model.predict(x_std_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "precision = precision_score(y_test,y_pred)\n",
        "recall = recall_score(y_test,y_pred)\n",
        "f2_scoree = fbeta_score(y_test,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDdzx_WXeAzW",
        "outputId": "922f431b-9a99-4067-90a0-623150fd59eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data Details : accuracy = 0.9767638888888889 , precision = 0.9921338960181437 , recall = 0.9610934979698538 , f2_scoree = 0.9671452159068649\n",
            "Test data Details : accuracy = 0.9636666666666667 , precision = 0.9809633027522936 , recall = 0.9460296394602964 , f2_scoree = 0.952815897344502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_small = pd.read_csv('/content/dataset_small.csv')\n",
        "df_small = df_small.drop(skipped_features,axis=1)\n",
        "df_small = df_small.drop(columns=drop_columns)\n",
        "x_t = df_small.drop('phishing',axis=1)\n",
        "y_t = df_small['phishing']\n",
        "\n",
        "pca_xt = pca.transform(x_t)\n",
        "std_xt = std.transform(pca_xt)\n",
        "\n",
        "#Validate details\n",
        "y_pred = rf_model.predict(std_xt)\n",
        "accuracy = accuracy_score(y_t,y_pred)\n",
        "precision = precision_score(y_t,y_pred)\n",
        "recall = recall_score(y_t,y_pred)\n",
        "f2_scoree = fbeta_score(y_t,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpUhlxTihmNt",
        "outputId": "9cba89f0-cf71-4da6-aa08-903d913046d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data Details : accuracy = 0.918663142637906 , precision = 0.9388163472952349 , recall = 0.9032205436094887 , f2_scoree = 0.9101221124065416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(base_score=0.5, booster=None, callbacks=None,\n",
        "              colsample_bylevel=0.7411992060554791, colsample_bynode=None,\n",
        "              colsample_bytree=0.6644703736569552, device=None,\n",
        "              early_stopping_rounds=None, enable_categorical=False,\n",
        "              eval_metric=None, feature_types=None, gamma=0.0295687727673445,\n",
        "              grow_policy=None, importance_type=None,\n",
        "              interaction_constraints=None, learning_rate=0.08628276003151057,\n",
        "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
        "              max_delta_step=0, max_depth=9, max_leaves=None,\n",
        "              min_child_weight=1, monotone_constraints=None,\n",
        "              multi_strategy=None, n_estimators=2800, n_jobs=1,\n",
        "              num_parallel_tree=None, random_state=None)\n",
        "\n",
        "xgb_model.fit(x_std_train, y_train)\n",
        "\n",
        "#Train details\n",
        "y_pred = xgb_model.predict(x_std_train)\n",
        "accuracy = accuracy_score(y_train,y_pred)\n",
        "precision = precision_score(y_train,y_pred)\n",
        "recall = recall_score(y_train,y_pred)\n",
        "f2_scoree = fbeta_score(y_train,y_pred,beta=2)\n",
        "\n",
        "print(f'Train data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n",
        "\n",
        "#Test details\n",
        "y_pred = xgb_model.predict(x_std_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "precision = precision_score(y_test,y_pred)\n",
        "recall = recall_score(y_test,y_pred)\n",
        "f2_scoree = fbeta_score(y_test,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n",
        "\n",
        "df_small = pd.read_csv('/content/dataset_small.csv')\n",
        "df_small = df_small.drop(skipped_features,axis=1)\n",
        "df_small = df_small.drop(columns=drop_columns)\n",
        "x_t = df_small.drop('phishing',axis=1)\n",
        "y_t = df_small['phishing']\n",
        "\n",
        "pca_xt = pca.transform(x_t)\n",
        "std_xt = std.transform(pca_xt)\n",
        "\n",
        "#Validate details\n",
        "y_pred = rf_model.predict(std_xt)\n",
        "accuracy = accuracy_score(y_t,y_pred)\n",
        "precision = precision_score(y_t,y_pred)\n",
        "recall = recall_score(y_t,y_pred)\n",
        "f2_scoree = fbeta_score(y_t,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GCt0LMMz06a",
        "outputId": "f0f3ff08-4ec2-4ec4-dd1c-50b3c41c320f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data Details : accuracy = 0.9753194444444444 , precision = 0.9965137561372418 , recall = 0.9539184604260527 , f2_scoree = 0.9621436946362752\n",
            "Test data Details : accuracy = 0.9625555555555556 , precision = 0.9905041031652989 , recall = 0.9344171643441717 , f2_scoree = 0.9451205870508748\n",
            "Test data Details : accuracy = 0.918663142637906 , precision = 0.9388163472952349 , recall = 0.9032205436094887 , f2_scoree = 0.9101221124065416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier(algorithm='kd_tree', leaf_size=22, n_jobs=1, n_neighbors=2,\n",
        "                     p=2.372428226692859, weights='distance')\n",
        "\n",
        "knn_model.fit(x_std_train, y_train)\n",
        "\n",
        "#Train details\n",
        "y_pred = knn_model.predict(x_std_train)\n",
        "accuracy = accuracy_score(y_train,y_pred)\n",
        "precision = precision_score(y_train,y_pred)\n",
        "recall = recall_score(y_train,y_pred)\n",
        "f2_scoree = fbeta_score(y_train,y_pred,beta=2)\n",
        "\n",
        "print(f'Train data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n",
        "\n",
        "#Test details\n",
        "y_pred = knn_model.predict(x_std_test)\n",
        "accuracy = accuracy_score(y_test,y_pred)\n",
        "precision = precision_score(y_test,y_pred)\n",
        "recall = recall_score(y_test,y_pred)\n",
        "f2_scoree = fbeta_score(y_test,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n",
        "\n",
        "df_small = pd.read_csv('/content/dataset_small.csv')\n",
        "df_small = df_small.drop(skipped_features,axis=1)\n",
        "df_small = df_small.drop(columns=drop_columns)\n",
        "x_t = df_small.drop('phishing',axis=1)\n",
        "y_t = df_small['phishing']\n",
        "\n",
        "pca_xt = pca.transform(x_t)\n",
        "std_xt = std.transform(pca_xt)\n",
        "\n",
        "#Validate details\n",
        "y_pred = knn_model.predict(std_xt)\n",
        "accuracy = accuracy_score(y_t,y_pred)\n",
        "precision = precision_score(y_t,y_pred)\n",
        "recall = recall_score(y_t,y_pred)\n",
        "f2_scoree = fbeta_score(y_t,y_pred,beta=2)\n",
        "print(f'Test data Details : accuracy = {accuracy} , precision = {precision} , recall = {recall} , f2_scoree = {f2_scoree}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hrdcblF0lob",
        "outputId": "1a19cf14-64cb-4208-ddb8-92faa9720950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data Details : accuracy = 0.9753194444444444 , precision = 0.9942164772562968 , recall = 0.9561432782690917 , f2_scoree = 0.9635228374613958\n",
            "Test data Details : accuracy = 0.9592222222222222 , precision = 0.9882463563704749 , recall = 0.9298827692988277 , f2_scoree = 0.9409974035276211\n",
            "Test data Details : accuracy = 0.9308892488703214 , precision = 0.981391644341467 , recall = 0.8845237706790224 , f2_scoree = 0.902336728579988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import joblib\n",
        "import socket\n",
        "\n",
        "def is_ip_address(domain):\n",
        "    try:\n",
        "        socket.inet_aton(domain)\n",
        "        return True\n",
        "    except socket.error:\n",
        "        return False\n",
        "\n",
        "def extract_additional_url_features(url):\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    return {\n",
        "        'qty_dot_url': url.count('.'),\n",
        "        'qty_hyphen_url': url.count('-'),\n",
        "        'qty_underline_url': url.count('_'),\n",
        "        'qty_slash_url': url.count('/'),\n",
        "        'qty_questionmark_url': url.count('?'),\n",
        "        'qty_equal_url': url.count('='),\n",
        "        'qty_at_url': url.count('@'),\n",
        "        'qty_exclamation_url': url.count('!'),\n",
        "        'qty_space_url': url.count(' '),\n",
        "        'qty_tilde_url': url.count('~'),\n",
        "        'qty_comma_url': url.count(','),\n",
        "        'qty_plus_url': url.count('+'),\n",
        "        'qty_asterisk_url': url.count('*'),\n",
        "        'qty_hashtag_url': url.count('#'),\n",
        "        'qty_dollar_url': url.count('$'),\n",
        "        'qty_percent_url': url.count('%'),\n",
        "        'qty_tld_url': len(parsed_url.netloc.split('.')[-1]),\n",
        "        'length_url': len(url)\n",
        "    }\n",
        "\n",
        "def extract_additional_domain_features(url):\n",
        "    # Parse the URL to get the domain\n",
        "    domain = urlparse(url).netloc\n",
        "\n",
        "    if not domain:\n",
        "        return {\n",
        "            'qty_dot_domain': -1,\n",
        "            'qty_hyphen_domain': -1,\n",
        "            'qty_underline_domain': -1,\n",
        "            'qty_at_domain': -1,\n",
        "            'qty_vowels_domain': -1,\n",
        "            'domain_in_ip': -1,\n",
        "            'server_client_domain': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'qty_dot_domain': domain.count('.'),\n",
        "        'qty_hyphen_domain': domain.count('-'),\n",
        "        'qty_underline_domain': domain.count('_'),\n",
        "        'qty_at_domain': domain.count('@'),\n",
        "        'qty_vowels_domain': sum(1 for char in domain if char.lower() in \"aeiou\"),\n",
        "        'domain_in_ip': 1 if is_ip_address(domain) else 0,\n",
        "        'server_client_domain': 1 if domain.startswith(\"www.\") else 0\n",
        "    }\n",
        "\n",
        "def extract_additional_path_features(url):\n",
        "    # Parse the URL to get the path\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    if not path:\n",
        "        return {\n",
        "            'qty_dot_directory': -1,\n",
        "            'qty_hyphen_directory': -1,\n",
        "            'qty_underline_directory': -1,\n",
        "            'qty_percent_directory': -1,\n",
        "            'directory_length': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'qty_dot_directory': path.count('.'),\n",
        "        'qty_hyphen_directory': path.count('-'),\n",
        "        'qty_underline_directory': path.count('_'),\n",
        "        'qty_percent_directory': path.count('%'),\n",
        "        'directory_length': len(path)\n",
        "    }\n",
        "\n",
        "def extract_file_features(url):\n",
        "    # Parse the URL to get the path\n",
        "    path = urlparse(url).path\n",
        "\n",
        "    if not path:\n",
        "        return {\n",
        "            'file_length': -1\n",
        "        }\n",
        "\n",
        "    # Extract the file name from the path\n",
        "    file_name = path.split('/')[-1]\n",
        "\n",
        "    # Attribute: Length of the file name\n",
        "    file_length = len(file_name)\n",
        "\n",
        "    return {\n",
        "        'file_length': file_length\n",
        "    }\n",
        "\n",
        "def extract_additional_params_features(url):\n",
        "    # Parse the URL to get the query parameters\n",
        "    query_params = urlparse(url).query\n",
        "\n",
        "    if not query_params:\n",
        "        return {\n",
        "            'qty_dot_params': -1,\n",
        "            'qty_hyphen_params': -1,\n",
        "            'qty_underline_params': -1,\n",
        "            'qty_slash_params': -1,\n",
        "            'qty_questionmark_params': -1,\n",
        "            'qty_percent_params': -1\n",
        "        }\n",
        "\n",
        "    # Extract parameter names from the query string\n",
        "    param_names = parse_qs(query_params).keys()\n",
        "\n",
        "    return {\n",
        "        'qty_dot_params': sum(param.count('.') for param in param_names),\n",
        "        'qty_hyphen_params': sum(param.count('-') for param in param_names),\n",
        "        'qty_underline_params': sum(param.count('_') for param in param_names),\n",
        "        'qty_slash_params': sum(param.count('/') for param in param_names),\n",
        "        'qty_questionmark_params': sum(param.count('?') for param in param_names),\n",
        "        'qty_percent_params': sum(param.count('%') for param in param_names)\n",
        "    }\n",
        "\n",
        "def email_urlshorten(url):\n",
        "    # Parse the URL\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Extract the domain from the URL\n",
        "    domain = parsed_url.netloc\n",
        "\n",
        "    if not domain:\n",
        "        return {\n",
        "            'email_in_url': -1,\n",
        "            'tls_ssl_certificate' : -1,\n",
        "            'url_shortened': -1\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        'email_in_url': 1 if '@' in url else 0,\n",
        "        'tls_ssl_certificate' : 1 if url.startswith(\"https://\") else 0,\n",
        "        'url_shortened': 1 if domain in ['bit.ly', 'goo.gl', 'tinyurl.com', 'ow.ly'] else 0\n",
        "    }\n",
        "\n",
        "def extract_all_features(url):\n",
        "    # Extract URL-based features\n",
        "    url_features = extract_additional_url_features(url)\n",
        "\n",
        "    # Extract Domain-based features\n",
        "    domain_features = extract_additional_domain_features(url)\n",
        "\n",
        "    # Extract Page-based features\n",
        "    path_features = extract_additional_path_features(url)\n",
        "\n",
        "    # Extract File-based feature\n",
        "    file_feature = extract_file_features(url)\n",
        "\n",
        "    # Extract Params-based features\n",
        "    params_features = extract_additional_params_features(url)\n",
        "\n",
        "    # Extract Additional Features\n",
        "    additional_features = email_urlshorten(url)\n",
        "\n",
        "    # Combine all features\n",
        "    all_features = {**url_features, **domain_features, **path_features, **file_feature, **params_features, **additional_features}\n",
        "\n",
        "    return all_features\n"
      ],
      "metadata": {
        "id": "axl1ZduCqZzh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://tinyurl.com/SHEIN-420\"\n",
        "# https://google.com\n",
        "# http://app.validchk.com/visitqr.aspx?vid=1073653\n",
        "# https://platform.openai.com/docs/overview\n",
        "# https://bard.google.com/chat/504c612c047ad681\n",
        "# https://chat.openai.com/c/e6ed6f21-d91a-45e8-b1ab-526589713026\n",
        "extracted_features = extract_all_features(url)\n",
        "\n",
        "# Extract features and reshape into a 2D array\n",
        "data = np.array(list(extracted_features.values())).reshape(1, -1)\n",
        "\n",
        "# Assuming you have a PCA object\n",
        "pca_transformed_data = pca.transform(data)\n",
        "\n",
        "# Assuming you have a scaler object\n",
        "scaled_data = std.transform(pca_transformed_data)\n",
        "\n",
        "\n",
        "# Use the trained XGBBoost for prediction\n",
        "#prediction = rf_model.predict(scaled_data)\n",
        "pd2 = knn_model.predict(scaled_data)\n",
        "pd3 = xgb_model.predict(scaled_data)\n",
        "print(prediction,pd2,pd3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "pralEmdyp2dI",
        "outputId": "ea63a0af-34e7-49eb-b079-c3bf77969359"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but PCA was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'knn_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-03e5207d28e6>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Use the trained XGBBoost for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#prediction = rf_model.predict(scaled_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mpd3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpd3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'knn_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from hpsklearn import HyperoptEstimator,any_preprocessing,random_forest_classifier,extra_tree_classifier,bagging_classifier,ada_boost_classifier,gradient_boosting_classifier,hist_gradient_boosting_classifier,ridge_classifier_cv,perceptron,decision_tree_classifier,k_neighbors_classifier,xgboost_classification\n",
        "from hyperopt import tpe\n",
        "\n",
        "estim = HyperoptEstimator(\n",
        "            classifier=k_neighbors_classifier('my_knn'),\n",
        "            preprocessing=any_preprocessing('my_pre'),\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=100,\n",
        "            trial_timeout=120,\n",
        "        )\n",
        "\n",
        "        # Fit the estimator\n",
        "estim.fit(x_std_train, y_train)\n",
        "\n",
        "# Get the best parameters and score\n",
        "best_params = estim._best_learner\n",
        "best_score = estim._best_loss\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = estim.predict(x_std_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f2_score = fbeta_score(y_test, y_pred, beta=2)"
      ],
      "metadata": {
        "id": "3QzVWDP5qtmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d7b3c85-56b2-4e68-8837-290b621512bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 1/1 [00:14<00:00, 14.61s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 2/2 [01:21<00:00, 81.81s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 3/3 [00:12<00:00, 12.46s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 4/4 [00:07<00:00,  7.40s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 5/5 [00:00<00:00,  1.81trial/s, best loss: 0.050694444444444486]\n",
            "100%|██████████| 6/6 [00:06<00:00,  6.33s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 7/7 [00:13<00:00, 13.40s/trial, best loss: 0.050694444444444486]\n",
            "100%|██████████| 8/8 [00:00<00:00,  1.73trial/s, best loss: 0.0500694444444445]\n",
            "100%|██████████| 9/9 [00:01<00:00,  1.10s/trial, best loss: 0.0500694444444445]\n",
            "100%|██████████| 10/10 [00:00<00:00,  2.21trial/s, best loss: 0.04861111111111116]\n",
            "100%|██████████| 11/11 [00:01<00:00,  1.56s/trial, best loss: 0.04861111111111116]\n",
            "100%|██████████| 12/12 [00:04<00:00,  4.75s/trial, best loss: 0.04861111111111116]\n",
            "100%|██████████| 13/13 [00:04<00:00,  4.92s/trial, best loss: 0.04861111111111116]\n",
            "100%|██████████| 14/14 [00:00<00:00,  1.75trial/s, best loss: 0.04861111111111116]\n",
            "100%|██████████| 15/15 [00:00<00:00,  1.48trial/s, best loss: 0.04861111111111116]\n",
            "100%|██████████| 16/16 [00:11<00:00, 11.80s/trial, best loss: 0.04861111111111116]\n",
            "100%|██████████| 17/17 [00:12<00:00, 12.47s/trial, best loss: 0.046666666666666634]\n",
            "100%|██████████| 18/18 [00:10<00:00, 10.16s/trial, best loss: 0.046666666666666634]\n",
            "100%|██████████| 19/19 [00:01<00:00,  1.36s/trial, best loss: 0.046666666666666634]\n",
            "100%|██████████| 20/20 [00:00<00:00,  1.13trial/s, best loss: 0.046666666666666634]\n",
            "100%|██████████| 21/21 [00:00<00:00,  3.89trial/s, best loss: 0.046666666666666634]\n",
            "100%|██████████| 22/22 [00:12<00:00, 12.46s/trial, best loss: 0.046666666666666634]\n",
            "100%|██████████| 23/23 [00:00<00:00,  1.67trial/s, best loss: 0.046666666666666634]\n",
            "100%|██████████| 24/24 [00:00<00:00,  1.69trial/s, best loss: 0.04500000000000004]\n",
            "100%|██████████| 25/25 [00:00<00:00,  1.69trial/s, best loss: 0.04500000000000004]\n",
            "100%|██████████| 26/26 [00:00<00:00,  1.71trial/s, best loss: 0.04500000000000004]\n",
            "100%|██████████| 27/27 [00:00<00:00,  1.72trial/s, best loss: 0.04500000000000004]\n",
            "100%|██████████| 28/28 [00:00<00:00,  2.25trial/s, best loss: 0.04500000000000004]\n",
            "100%|██████████| 29/29 [00:01<00:00,  1.26s/trial, best loss: 0.04500000000000004]\n",
            "100%|██████████| 30/30 [00:01<00:00,  1.02s/trial, best loss: 0.04500000000000004]\n",
            "100%|██████████| 31/31 [00:00<00:00,  1.69trial/s, best loss: 0.04472222222222222]\n",
            "100%|██████████| 32/32 [00:01<00:00,  1.36s/trial, best loss: 0.04472222222222222]\n",
            "100%|██████████| 33/33 [00:00<00:00,  1.45trial/s, best loss: 0.04472222222222222]\n",
            "100%|██████████| 34/34 [00:01<00:00,  1.17s/trial, best loss: 0.04472222222222222]\n",
            "100%|██████████| 35/35 [00:00<00:00,  1.37trial/s, best loss: 0.04472222222222222]\n",
            "100%|██████████| 36/36 [00:04<00:00,  4.37s/trial, best loss: 0.04472222222222222]\n",
            "100%|██████████| 37/37 [00:00<00:00,  1.24trial/s, best loss: 0.04472222222222222]\n",
            "100%|██████████| 38/38 [00:00<00:00,  2.68trial/s, best loss: 0.04472222222222222]\n",
            "100%|██████████| 39/39 [00:00<00:00,  3.12trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 40/40 [00:03<00:00,  3.08s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 41/41 [00:00<00:00,  2.71trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 42/42 [00:02<00:00,  2.02s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 43/43 [00:00<00:00,  1.48trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 44/44 [00:01<00:00,  1.94s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 45/45 [00:12<00:00, 12.30s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 46/46 [00:00<00:00,  1.59trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 47/47 [00:00<00:00,  2.49trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 48/48 [00:04<00:00,  4.07s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 49/49 [00:12<00:00, 12.36s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 50/50 [00:00<00:00,  3.95trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 51/51 [00:01<00:00,  1.37s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 52/52 [00:10<00:00, 10.21s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 53/53 [00:00<00:00,  1.15trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 54/54 [00:05<00:00,  5.23s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 55/55 [00:00<00:00,  2.76trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 56/56 [00:00<00:00,  3.09trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 57/57 [00:12<00:00, 12.32s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 58/58 [00:00<00:00,  1.32trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 59/59 [00:04<00:00,  4.00s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 60/60 [00:00<00:00,  1.99trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 61/61 [00:12<00:00, 12.20s/trial, best loss: 0.0443055555555556]\n",
            "100%|██████████| 62/62 [00:00<00:00,  1.07trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 63/63 [00:00<00:00,  1.44trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 64/64 [00:00<00:00,  3.92trial/s, best loss: 0.0443055555555556]\n",
            "100%|██████████| 65/65 [00:00<00:00,  2.02trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 66/66 [00:00<00:00,  1.61trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 67/67 [00:00<00:00,  1.91trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 68/68 [00:00<00:00,  1.48trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 69/69 [00:00<00:00,  1.90trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 70/70 [00:00<00:00,  2.48trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 71/71 [00:00<00:00,  3.04trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 72/72 [00:00<00:00,  4.03trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 73/73 [00:00<00:00,  2.92trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 74/74 [00:00<00:00,  3.95trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 75/75 [00:00<00:00,  1.63trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 76/76 [00:00<00:00,  1.53trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 77/77 [00:00<00:00,  2.48trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 78/78 [00:00<00:00,  2.14trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 79/79 [00:00<00:00,  2.38trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 80/80 [00:01<00:00,  1.52s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 81/81 [00:00<00:00,  1.86trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 82/82 [00:00<00:00,  2.13trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 83/83 [00:00<00:00,  3.03trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 84/84 [00:00<00:00,  3.05trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 85/85 [00:00<00:00,  1.04trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 86/86 [00:00<00:00,  1.09trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 87/87 [00:00<00:00,  1.58trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 88/88 [00:05<00:00,  5.99s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 89/89 [00:01<00:00,  1.21s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 90/90 [00:00<00:00,  3.86trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 91/91 [00:12<00:00, 12.32s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 92/92 [00:00<00:00,  4.39trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 93/93 [00:06<00:00,  6.13s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 94/94 [00:00<00:00,  1.42trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 95/95 [00:09<00:00,  9.86s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 96/96 [00:00<00:00,  1.21trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 97/97 [00:00<00:00,  3.27trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 98/98 [00:02<00:00,  2.33s/trial, best loss: 0.04409722222222223]\n",
            "100%|██████████| 99/99 [00:00<00:00,  1.46trial/s, best loss: 0.04409722222222223]\n",
            "100%|██████████| 100/100 [00:12<00:00, 12.34s/trial, best loss: 0.04409722222222223]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "xfwz9UdhpI7w",
        "outputId": "3ef08e07-2f05-422f-b079-7fed54105ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='kd_tree', leaf_size=25, metric='cityblock',\n",
              "                     n_jobs=1, n_neighbors=2, p=2.2992555541721913,\n",
              "                     weights='distance')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(algorithm=&#x27;kd_tree&#x27;, leaf_size=25, metric=&#x27;cityblock&#x27;,\n",
              "                     n_jobs=1, n_neighbors=2, p=2.2992555541721913,\n",
              "                     weights=&#x27;distance&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;KNeighborsClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">?<span>Documentation for KNeighborsClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>KNeighborsClassifier(algorithm=&#x27;kd_tree&#x27;, leaf_size=25, metric=&#x27;cityblock&#x27;,\n",
              "                     n_jobs=1, n_neighbors=2, p=2.2992555541721913,\n",
              "                     weights=&#x27;distance&#x27;)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ1IbaFYrXDl",
        "outputId": "9e2525f3-90bb-4018-980f-2a34c348d090"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNeighborsClassifier(algorithm='kd_tree', leaf_size=25, metric='cityblock',\n",
            "                     n_jobs=1, n_neighbors=2, p=2.2992555541721913,\n",
            "                     weights='distance')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "9aOPFaxJrcKo",
        "outputId": "d5e6cf11-9c60-4362-ae03-bb2e1df3ffad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'KNeighborsClassifier' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-7031ef46afcd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algorithm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'KNeighborsClassifier' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "teDT_u6etW0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}